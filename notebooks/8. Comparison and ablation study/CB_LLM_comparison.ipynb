{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "468de2bb-d256-4a1f-9426-ff879493593e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Launch CB-LLM\n",
    "\n",
    "---\n",
    "Goal of the notebook: .\n",
    "\n",
    "Inputs of the notebook:\n",
    "-\n",
    "Output of the notebook:\n",
    "-\n",
    "Takeaways: \n",
    "- .\n",
    "- ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../run_experiments/')\n",
    "sys.path.append('../../run_experiments/scripts')\n",
    "sys.path.append('../../run_experiments/models')\n",
    "sys.path.append('../../run_experiments/data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2461aae4-df2c-4a99-bc2d-a095c139a183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pickle \n",
    "\n",
    "# import fonction for getting PLM and tokenizer\n",
    "from models.utils import load_model_and_tokenizer\n",
    "\n",
    "\n",
    "# library for managing memory RAM\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e68d10-f08b-4575-b2e7-cb37fdb37df0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.SETUP ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import config\n",
    "from load_config import load_config\n",
    "\n",
    "model_name = 'bert-base-uncased'    # 'bert-base-uncased' ou 'deberta-large' or 'gemma'\n",
    "dataset    = 'movies'               # 'movies' / 'agnews' / 'dbpedia' / 'medical'/ 'ledgar'/ n24news\n",
    "annotation = 'C3M'       # 'C3M' ou 'our_annotation' ou 'combined_annotation'\n",
    "config = load_config(model_name, dataset)\n",
    "config.annotation = annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, _ , _ = load_model_and_tokenizer(config, n_concepts = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d6f9e3-5dc4-4273-9fbc-1c3656f14e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare your dataset (replace with your actual dataset)\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, concepts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.concepts = concepts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.concepts is not None:\n",
    "            concept = self.concepts[idx]\n",
    "            encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'concepts': torch.tensor(concept, dtype=torch.float),\n",
    "                'label': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        else :\n",
    "            encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'label': torch.tensor(label, dtype=torch.long)\n",
    "            }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee17e3a-0224-461c-8bbb-877ce41e8cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_aug_train = pd.read_csv(f\"{config.SAVE_PATH_CONCEPTS}/df_with_topics_v4.csv\")\n",
    "df_aug_test = pd.read_csv(f\"{config.SAVE_PATH_CONCEPTS}/df_with_topics_v4_test.csv\")\n",
    "df_aug_train_cbllm_acc = pd.read_csv(f\"{config.SAVE_PATH_CONCEPTS}/df_with_topics_v4_CB_LLM_ACC.csv\")\n",
    "\n",
    "columns = [c for c in df_aug_train_cbllm_acc.columns if \"dummy\" not in c and c not in [\"Unnamed: 0.1\", \"Unnamed: 0\", \"test\"]]\n",
    "df_aug_train_cbllm_acc = df_aug_train_cbllm_acc[columns]\n",
    "concept_col = df_aug_train_cbllm_acc.columns[2:]\n",
    "train_concepts = df_aug_train_cbllm_acc[concept_col].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8fafd3-fbad-4573-ba64-a7d7a2b8440d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = TextClassificationDataset(df_aug_train_cbllm_acc.text, df_aug_train_cbllm_acc.label, train_concepts, tokenizer, max_length=128)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "test_dataset = TextClassificationDataset(df_aug_test.text, df_aug_test.label, train_concepts, tokenizer, max_length=128)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a426bb6-e020-46f1-a352-05727098f45f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Entrainemet concept seulement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "777fde56-1144-4500-b98f-a762598a9a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "code below used for xp on bert without residual part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83dd4d1-9fb2-43e3-b412-b98f122cdffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CBL(nn.Module):\n",
    "    def __init__(self, base_model, tokenizer, concept_dim, max_length, l1_ratio=0.99, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        import copy\n",
    "        self.base_model = copy.deepcopy(base_model)\n",
    "        for p in self.base_model.parameters():\n",
    "            p.requires_grad = True\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concept_extractor = nn.Linear(base_model.config.hidden_size, concept_dim)\n",
    "        self.max_length = max_length\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(concept_dim, concept_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state[:, 0, :]\n",
    "        concepts = F.relu(self.concept_extractor(last_hidden_state)) #here\n",
    "        # concepts = F.relu(self.concept_extractor(last_hidden_state)) #here\n",
    "        # concepts = self.gelu(self.concept_extractor(last_hidden_state)) #here\n",
    "        concepts = self.fc(concepts) #here\n",
    "        concepts = self.dropout(concepts) #here\n",
    "        concepts = concepts + self.concept_extractor(last_hidden_state) #here\n",
    "        return concepts\n",
    "    \n",
    "    def get_regularization(self):\n",
    "        return self.classifier.regularization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0f3fc0-6b66-407e-b7aa-9db0e462532b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#trouvé dans le code source dez CB_LLM\n",
    "def cos_sim_cubed(cbl_features, target):\n",
    "    cbl_features = cbl_features - torch.mean(cbl_features, dim=-1, keepdim=True)\n",
    "    target = target - torch.mean(target, dim=-1, keepdim=True)\n",
    "\n",
    "    cbl_features = F.normalize(cbl_features**3, dim=-1)\n",
    "    target = F.normalize(target**3, dim=-1)\n",
    "\n",
    "    sim = torch.sum(cbl_features*target, dim=-1)\n",
    "    return sim.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f143e1f4-cdbf-4451-b639-4d6157b269cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "concept_dim = df_aug_train_cbllm_acc.shape[1]-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc82f72c-6688-48cc-b28a-379e1a72ecd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = TextClassificationDataset(df_aug_train_cbllm_acc.text, df_aug_train_cbllm_acc.label, train_concepts, tokenizer, max_length=128)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_dataset = TextClassificationDataset(df_aug_test.text, df_aug_test.label, train_concepts, tokenizer, max_length=128)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2282442c-7527-485c-b245-7f88b78bf7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialisation de la meilleure loss à un très grand nombre\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Hyperparameters\n",
    "concept_dim = df_aug_train_cbllm_acc.shape[1]-2  # Number of concepts\n",
    "\n",
    "# num_classes = df_aug_train_cbllm_acc.label.nunique()  # Number of classes\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "# Initialize model, loss functions, and optimizer\n",
    "cbl = CBL(model, tokenizer, concept_dim, max_length)\n",
    "cbl.to(config.device)\n",
    "\n",
    "# concept_criterion = nn.MSELoss()\n",
    "concept_criterion = cos_sim_cubed\n",
    "optimizer = optim.Adam(cbl.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    cbl.train()\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(config.device)\n",
    "        attention_mask = batch['attention_mask'].to(config.device)\n",
    "        true_concepts = batch['concepts'].to(config.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predicted_concepts = cbl(input_ids, attention_mask)\n",
    "\n",
    "        # Compute losses\n",
    "        concept_loss = concept_criterion(predicted_concepts, true_concepts)\n",
    "        \n",
    "        # total_loss = concept_loss \n",
    "        total_loss = -concept_loss \n",
    "\n",
    "        # print(\"classiff_loss\", classification_loss)\n",
    "        # print(\"-------------------\")\n",
    "        # print(\"concept_loss\", concept_loss)\n",
    "        # Backward pass and optimization\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        del input_ids, attention_mask, true_concepts, predicted_concepts, \n",
    "\n",
    "    # Sauvegarde du meilleur modèle\n",
    "    if total_loss.item() < best_loss:\n",
    "        best_loss = total_loss.item()\n",
    "        torch.save(cbl.state_dict(), f'{config.SAVE_PATH}/cbl_{config.model_name}_best_model_iteration.pth')\n",
    "        print(f\"New best model saved with loss: {best_loss:.4f}\")\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32fa461f-b5b2-46b1-a965-e790eccc628c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "torch.save(cbl.state_dict(), f'{config.SAVE_PATH}/cbl_{config.model_name}_last_model_iteration.pth')\n",
    "print(f\"Last model saved with loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92519427-6646-4438-a178-030debbe247f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Re-créer l'instance du modèle\n",
    "cbl = CBL(model, tokenizer, concept_dim, max_length)\n",
    "cbl.load_state_dict(torch.load(f'{config.SAVE_PATH}/cbl_{config.model_name}_best_model_iteration.pth'))\n",
    "cbl.to(config.device)\n",
    "cbl.eval()  # Passage en mode évaluation si nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78a92d38-2dbe-4fb2-87bf-6049dd532de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60037e4f-0a3d-46b8-bac2-6570dc3d90c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ElasticNetLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, l1_ratio=0.99):\n",
    "        super(ElasticNetLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.l1_ratio = l1_ratio\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def regularization(self):\n",
    "        l1_reg = torch.norm(self.linear.weight, p=1)\n",
    "        l2_reg = torch.norm(self.linear.weight, p=2)\n",
    "        return self.l1_ratio * l1_reg + (1 - self.l1_ratio) * l2_reg\n",
    "\n",
    "class CB_LLM(nn.Module):\n",
    "    def __init__(self, cbl_model, tokenizer, concept_dim, num_classes, max_length, l1_ratio=0.99, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.cbl_model = copy.deepcopy(cbl_model)\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.concept_extractor = nn.Linear(base_model.config.hidden_size, concept_dim)\n",
    "        self.max_length = max_length\n",
    "        self.classifier = ElasticNetLayer(concept_dim, num_classes, l1_ratio)\n",
    "\n",
    "        # Freeze parameters of cbl_model\n",
    "        for param in self.cbl_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        concepts = self.cbl_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = self.classifier(F.relu(concepts))\n",
    "        return concepts, output\n",
    "    \n",
    "    def get_regularization(self):\n",
    "        return self.classifier.regularization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8dc08e-b8b6-437c-a562-2e79f2bed928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = TextClassificationDataset(df_aug_train_cbllm_acc.text, df_aug_train_cbllm_acc.label, train_concepts, tokenizer, max_length=256)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "test_dataset = TextClassificationDataset(df_aug_test.text, df_aug_test.label, None, tokenizer, max_length=256)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab8183a-ef3d-4115-927a-cf3a24e28e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "concept_dim = df_aug_train_cbllm_acc.shape[1]-2  # Number of concepts\n",
    "num_classes = df_aug_train_cbllm_acc.label.nunique()  # Number of classes\n",
    "# learning_rate = 0.001\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_epochs = 20\n",
    "# lambda_concept = 1  # Weight for concept loss\n",
    "lambda_concept = 0.05  # Weight for concept loss\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "# Initialize model, loss functions, and optimizer\n",
    "cb_llm_model = CB_LLM(cbl_model = cbl, tokenizer = tokenizer, concept_dim = concept_dim, num_classes = num_classes, max_length = max_length)\n",
    "cb_llm_model.to(config.device)\n",
    "# concept_criterion = nn.MSELoss()\n",
    "classification_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cb_llm_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    cb_llm_model.train()\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(config.device)\n",
    "        attention_mask = batch['attention_mask'].to(config.device)\n",
    "        true_concepts = batch['concepts'].to(config.device)\n",
    "        labels = batch['label'].to(config.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predicted_concepts, predicted_output = cb_llm_model(input_ids, attention_mask)\n",
    "        \n",
    "        # Compute losses\n",
    "        # concept_loss = concept_criterion(predicted_concepts, true_concepts)\n",
    "        classification_loss = classification_criterion(predicted_output, labels)\n",
    "        regularization = cb_llm_model.get_regularization()\n",
    "        \n",
    "        # Joint loss\n",
    "        # total_loss = classification_loss + lambda_concept * concept_loss +  0.0007 * regularization\n",
    "        total_loss = classification_loss +  0.0007 * regularization\n",
    "        # print(\"classiff_loss\", classification_loss)\n",
    "        # print(\"-------------------\")\n",
    "        # print(\"concept_loss\", concept_loss)\n",
    "        # Backward pass and optimization\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    #Epoch eval\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss.item()}\")\n",
    "    cb_llm_model.eval()\n",
    "    accuracy = 0.\n",
    "    predict_labels = np.array([])\n",
    "    true_labels = np.array([])\n",
    "    for batch_eval in (test_dataloader):\n",
    "        input_ids_eval = batch_eval[\"input_ids\"].to(config.device)\n",
    "        attention_mask_eval = batch_eval[\"attention_mask\"].to(config.device)\n",
    "        label_eval = batch_eval[\"label\"].to(config.device)\n",
    "        logits = cb_llm_model(input_ids=input_ids_eval, attention_mask=attention_mask_eval)[1]\n",
    "        predictions = torch.argmax(logits, axis=1)\n",
    "        accuracy += torch.sum(predictions == label_eval).item()\n",
    "        predict_labels = np.append(predict_labels, predictions.cpu().numpy())\n",
    "        true_labels = np.append(true_labels, label_eval.cpu().numpy())\n",
    "\n",
    "        # Libérer la mémoire GPU après chaque batch\n",
    "        del input_ids_eval, attention_mask_eval, label_eval, logits, predictions\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    accuracy /= len(test_dataloader.dataset)\n",
    "    num_true_labels = len(np.unique(true_labels))\n",
    "    macro_f1_scores = []\n",
    "    for label in range(num_true_labels):\n",
    "        label_pred = np.array(predict_labels) == label\n",
    "        label_true = np.array(true_labels) == label\n",
    "        macro_f1_scores.append(f1_score(label_true, label_pred, average='macro'))\n",
    "    mean_macro_f1_score = np.mean(macro_f1_scores)\n",
    "    print(f\"Acc = {accuracy*100} Macro F1 = {mean_macro_f1_score*100}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Acc = {accuracy*100} Macro F1 = {mean_macro_f1_score*100}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d5a91c-44d8-49b4-8b6c-092b9068ea45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1152145-3be6-4121-880a-5b3bf81b4834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save en pth cb_llm_model et cbl\n",
    "torch.save(cbl.state_dict(), f'{config.SAVE_PATH}/cbl_{config.model_name}.pth')\n",
    "torch.save(cb_llm_model.state_dict(), f'{config.SAVE_PATH}/cb_llm_model_{config.model_name}.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que sont déjà importées ou définies nos classes : CBL et CB_LLM\n",
    "\n",
    "# Hyperparameters\n",
    "concept_dim = df_aug_train_cbllm_acc.shape[1]-2  # Number of concepts\n",
    "\n",
    "# num_classes = df_aug_train_cbllm_acc.label.nunique()  # Number of classes\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "num_epochs = 4\n",
    "\n",
    "max_length = 512 # ne sert à rien en vrai\n",
    "\n",
    "# Initialize model, loss functions, and optimizer\n",
    "cbl = CBL(model, tokenizer, concept_dim, max_length)\n",
    "cbl.to(config.device)\n",
    "\n",
    "# Chargement des poids sauvegardés pour 'cbl'\n",
    "cbl_state = torch.load(f'{config.SAVE_PATH}/cb_llm_checkpoints/cbl_{config.model_name}.pth', map_location=config.device)    \n",
    "cbl.load_state_dict(cbl_state)\n",
    "cbl.to(config.device)\n",
    "cbl.eval()\n",
    "\n",
    "print(\"Modèles chargés et prêts pour l'évaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Supposons que vous ayez récupéré les noms des concepts depuis votre DataFrame\n",
    "# Par exemple, si les deux premières colonnes ne sont pas des concepts :\n",
    "concept_names = list(df_aug_train_cbllm_acc.columns[2:])  # Liste des noms de concepts\n",
    "\n",
    "##############################################\n",
    "# 1. Calculer la médiane sur les valeurs réelles du train\n",
    "##############################################\n",
    "true_concepts_train_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_dataloader:\n",
    "        # On récupère les valeurs réelles des concepts\n",
    "        true_concepts = batch[\"concepts\"].to(config.device)  # Forme attendue : [batch_size, num_concepts]\n",
    "        true_concepts_train_all.append(true_concepts.cpu().numpy())\n",
    "\n",
    "# Concaténation sur toutes les batchs\n",
    "true_concepts_train_all = np.concatenate(true_concepts_train_all, axis=0)  # Forme : (N_train, num_concepts)\n",
    "\n",
    "# Calcul de la médiane pour chaque concept (le long de l'axe 0)\n",
    "median_per_concept = np.median(true_concepts_train_all, axis=0)\n",
    "\n",
    "print(\"Médianes calculées sur le jeu d'entraînement :\")\n",
    "for i, median_val in enumerate(median_per_concept):\n",
    "    concept_name = concept_names[i] if i < len(concept_names) else f\"Concept {i}\"\n",
    "    print(f\"  {concept_name}: {median_val:.3f}\")\n",
    "\n",
    "##############################################\n",
    "# 2. Évaluation sur un autre jeu (par exemple, le test)\n",
    "##############################################\n",
    "predicted_concepts_all = []\n",
    "true_concepts_all = []\n",
    "\n",
    "cbl.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(config.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(config.device)\n",
    "        true_concepts = batch[\"concepts\"].to(config.device)  # Taille : [batch_size, num_concepts]\n",
    "        \n",
    "        # Obtenir les concepts prédits (le second output est ignoré ici)\n",
    "        predicted_concepts = cbl(input_ids, attention_mask)\n",
    "\n",
    "        # Déplacement sur CPU et conversion en numpy\n",
    "        predicted_concepts_all.append(predicted_concepts.cpu().numpy())\n",
    "        true_concepts_all.append(true_concepts.cpu().numpy())\n",
    "\n",
    "predicted_concepts_all = np.concatenate(predicted_concepts_all, axis=0)  # Forme : (N_test, num_concepts)\n",
    "true_concepts_all = np.concatenate(true_concepts_all, axis=0)            # Forme : (N_test, num_concepts)\n",
    "\n",
    "num_concepts = predicted_concepts_all.shape[1]\n",
    "f1_scores = []\n",
    "\n",
    "# Pour chaque concept, on binarise avec la médiane calculée sur le train\n",
    "for i in range(num_concepts):\n",
    "    median_value = median_per_concept[i]  # Seuil défini par le train pour le concept i\n",
    "    \n",
    "    # Récupérer les valeurs pour le concept i sur l'ensemble d'évaluation\n",
    "    pred_values = predicted_concepts_all[:, i]\n",
    "    true_values = true_concepts_all[:, i]\n",
    "    \n",
    "    # Binarisation avec le même seuil (la médiane du train)\n",
    "    pred_binary = (pred_values > median_value).astype(int)\n",
    "    true_binary = (true_values > median_value).astype(int)\n",
    "    \n",
    "    # Calcul du F1-score\n",
    "    f1 = f1_score(true_binary, pred_binary)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    concept_name = concept_names[i] if i < len(concept_names) else f\"Concept {i}\"\n",
    "    print(f\"Concept '{concept_name}': Médiane (train) = {median_value:.3f}, F1 score = {f1:.3f}\")\n",
    "\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "print(\"F1 score moyen sur tous les concepts :\", mean_f1)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(dbpedia_deberta) Fine_tuning CB_LLM",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
