{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0745a076-0599-4a14-a5ff-3e1b0ab4d383",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Computation of Cavs vectors\n",
    "\n",
    "---\n",
    "Goal of the notebook: .\n",
    "\n",
    "Inputs of the notebook:\n",
    "-\n",
    "Output of the notebook:\n",
    "-\n",
    "Takeaways: \n",
    "- .\n",
    "- ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b14703-f14b-46cf-af06-cc4193b66bd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../run_experiments/')\n",
    "sys.path.append('../../run_experiments/scripts')\n",
    "sys.path.append('../../run_experiments/models')\n",
    "sys.path.append('../../run_experiments/data')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# import fonction for getting PLM and tokenizer\n",
    "from models.utils import load_model_and_tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f921208-c689-4c81-b68f-22c4922c67ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 0. autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50839d65-be00-4f76-8e12-c122b7b69d6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#code for autoreload script associated with jupyter notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cavs creation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import config\n",
    "from load_config import load_config\n",
    "\n",
    "model_name = 'bert-base-uncased'    # 'bert-base-uncased' ou 'deberta-large'\n",
    "dataset    = 'movies'               # 'movies' / 'agnews' / 'dbpedia' / 'medical'/ 'ledgar'/ n24news\n",
    "annotation = 'C3M'       # 'C3M' ou 'our_annotation' ou 'combined_annotation'\n",
    "config = load_config(model_name, dataset)\n",
    "config.annotation = annotation\n",
    "\n",
    "## choisir le mode de calcul des cavs : changer le truc du fichier config pour enregristrer les fichiers dans le bon dossier\n",
    "# !!!!!!!! important !!!!!!!\n",
    "config.cavs_type = 'mean'\n",
    "# !!!!!!!! important !!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "from prepare_data import load_fc_prepare_data\n",
    "prepare_data = load_fc_prepare_data(config.DATASET)\n",
    "train_loader, test_loader, val_loader, train_df, val_df, test_df = prepare_data(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load backbone to compute activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model_name == 'gemma':\n",
    "    from models.BaselineModel_gemma import BaselineModel\n",
    "else:\n",
    "    from models.BaselineModel import BaselineModel\n",
    "# import the PLM model and tokenizer and bottleneck layer\n",
    "embedder_model, embedder_tokenizer, ModelXtoCtoY_layer, classifier = load_model_and_tokenizer(config, \n",
    "                                                                                              n_concepts = 4\n",
    "                                                                                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "black_box_model = BaselineModel(embedder_model, classifier, None, None, None, config, save_path = config.SAVE_PATH)\n",
    "black_box_model.load_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### charger les dataframes et dataloaders augmentés avec les concepts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepare_data import prepare_data_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug_train = prepare_data_from_csv(annotation = 'combined_annotation', config = config) # C3M our_annotation C3M combined_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concepts_bank_utils import create_dataloader\n",
    "notre_loader_train = create_dataloader(df_aug_train, embedder_tokenizer, config.max_len, config.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concepts_bank_utils import create_dataloader\n",
    "# charger les données de train (facultatif : validation et de test augmented)\n",
    "df_aug_train = pd.read_csv(f\"{config.SAVE_PATH_CONCEPTS}/df_with_topics_v4.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model_name == 'gemma':\n",
    "    from mean_cavs_creation_gemma import compute_cavs_mean_minus\n",
    "else:\n",
    "    from mean_cavs_creation import compute_cavs_mean_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul les cavs\n",
    "cavs = compute_cavs_mean_minus(\n",
    "    df_aug= df_aug_train,\n",
    "    embedder_model=black_box_model.embedder_model,\n",
    "    embedder_tokenizer=embedder_tokenizer,\n",
    "    baseline_model=black_box_model,\n",
    "    config=config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculer le TCAVS score à partir des moyennes des embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_name_list = []\n",
    "\n",
    "# Consolidation des concepts dans une colonne unique\n",
    "for column in df_aug_train.columns:\n",
    "    if 'dummy' not in column:\n",
    "        continue\n",
    "    concept_name = column.replace('dummy_', '')\n",
    "    concept_name_list.append(concept_name)\n",
    "\n",
    "print(len(concept_name_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IMPORTANT LIBRARY TO IMPORT\n",
    "from TCAVS import TCAV\n",
    "from TCAVS_utils import stratified_subset_dataloader\n",
    "\n",
    "TCAVS_ranker = TCAV(\n",
    "    concepts=concept_name_list,\n",
    "    baseline_model=black_box_model,\n",
    "    embedder_tokenizer = embedder_tokenizer,\n",
    "    batch_size=config.batch_size,\n",
    "    config=config,\n",
    "    verbose=True\n",
    ")    \n",
    "\n",
    "TCAVS_ranker.load_cavs_from_file()\n",
    "\n",
    "tcav_scores = TCAVS_ranker.calculate_tcav_scores(\n",
    "    notre_loader_train,\n",
    "     use_subset=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### moyenne pour toutes les classes et enregistrer en json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ranking_utils import rank_macro_concepts\n",
    "import json\n",
    "# # Rank the macro concepts based on TCAV scores\n",
    "sorted_macro_concepts = rank_macro_concepts(tcav_scores)\n",
    "\n",
    "file_path = f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs/{config.cavs_type}\"\n",
    "\n",
    "with open(f'{file_path}/sorted_macro_concepts.json', 'w') as f:\n",
    "     json.dump(sorted_macro_concepts, fp=f, default=lambda x: x.tolist())\n",
    "\n",
    "print('sorted macro concepts enregistré ici:', f'{file_path}/sorted_macro_concepts.json')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4_Joint_Incremental_CBM_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
