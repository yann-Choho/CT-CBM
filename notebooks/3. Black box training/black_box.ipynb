{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0745a076-0599-4a14-a5ff-3e1b0ab4d383",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Train and save Baseline model\n",
    "\n",
    "---\n",
    "Goal of the notebook: .\n",
    "\n",
    "Inputs of the notebook:\n",
    "-\n",
    "Output of the notebook:\n",
    "-\n",
    "Takeaways: \n",
    "- .\n",
    "- ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b14703-f14b-46cf-af06-cc4193b66bd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../run_experiments/')\n",
    "sys.path.append('../../run_experiments/scripts')\n",
    "sys.path.append('../../run_experiments/models')\n",
    "sys.path.append('../../run_experiments/data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for autoreload script associated with jupyter notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fonction for getting PLM and tokenizer\n",
    "from models.utils import load_model_and_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7110737a-9a5a-40fb-b645-5c9cd28f652e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1.SETUP ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6cae576-657f-403d-b4d4-e986af811158",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import config\n",
    "from load_config import load_config\n",
    "\n",
    "model_name = 'gemma'    # 'bert-base-uncased' ou 'deberta-large' or 'gemma'\n",
    "dataset    = 'movies'               # 'movies' / 'agnews' / 'dbpedia' / 'medical'/ 'ledgar'/ n24news\n",
    "annotation = 'combined_annotation'       # 'C3M' ou 'our_annotation' ou 'combined_annotation'\n",
    "config = load_config(model_name, dataset)\n",
    "config.annotation = annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35cdb74e-0d6f-4e0c-bfb4-18a7d429ae21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import the data\n",
    "from prepare_data import load_fc_prepare_data\n",
    "prepare_data = load_fc_prepare_data(config.DATASET)\n",
    "train_loader, test_loader, val_loader, train_df, val_df, test_df = prepare_data(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27a12c09-6d64-418f-b925-07228c137cd7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Launch All the blackbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model_name == 'gemma':\n",
    "    from models.BaselineModel_gemma import BaselineModel\n",
    "else:\n",
    "    from models.BaselineModel import BaselineModel\n",
    "\n",
    "# import the PLM model and tokenizer and bottleneck layer\n",
    "embedder_model, embedder_tokenizer, ModelXtoCtoY_layer, classifier = load_model_and_tokenizer(config, \n",
    "                                                                                              n_concepts = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_box_model = BaselineModel(embedder_model, classifier, train_loader, val_loader, test_loader, config, save_path = config.SAVE_PATH)\n",
    "black_box_model.train_model()\n",
    "black_box_model.load_model()\n",
    "black_box_model.evaluate_model(test_loader, 'Test')\n",
    "black_box_model.save_performance_json()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4_Joint_Incremental_CBM_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python (yann_kernel)",
   "language": "python",
   "name": "yann_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
