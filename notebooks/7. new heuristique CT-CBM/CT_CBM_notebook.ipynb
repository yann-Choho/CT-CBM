{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0745a076-0599-4a14-a5ff-3e1b0ab4d383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# CT-CBM \n",
    "\n",
    "---\n",
    "Goal of the notebook: .\n",
    "\n",
    "Inputs of the notebook:\n",
    "- \n",
    "\n",
    "Output of the notebook:\n",
    "- prediction + interpretation layer for text classification.\n",
    "\n",
    "\n",
    "Takeaways: \n",
    "- .\n",
    "- ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b14703-f14b-46cf-af06-cc4193b66bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61184a1f-5298-493b-bf64-b31f6b8d93ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../run_experiments/')\n",
    "sys.path.append('../../run_experiments/scripts')\n",
    "sys.path.append('../../run_experiments/models')\n",
    "sys.path.append('../../run_experiments/data')\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "\n",
    "# model for CBM\n",
    "# import fonction for getting PLM and tokenizer\n",
    "from models.utils import load_model_and_tokenizer\n",
    "\n",
    "from concepts_discovery_utils import extract_target_words, create_context_window, load_model, run_concepts_discovery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32e801e0-ef91-415b-bb0d-30d14bb5e61f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Delete awkward warning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4447ea53-7454-4a81-b658-69c2da1c90a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "for logger in loggers:\n",
    "    if \"transformers\" in logger.name.lower():\n",
    "        logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f921208-c689-4c81-b68f-22c4922c67ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 0. autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50839d65-be00-4f76-8e12-c122b7b69d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#code for autoreload script associated with jupyter notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7110737a-9a5a-40fb-b645-5c9cd28f652e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1.SETUP ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6cae576-657f-403d-b4d4-e986af811158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import config\n",
    "from load_config import load_config\n",
    "\n",
    "model_name = 'bert-base-uncased'    # 'bert-base-uncased' ou 'deberta-large' or 'gemma'\n",
    "dataset    = 'agnews'               # 'movies' / 'agnews' / 'dbpedia' / 'medical'/ 'ledgar'/ n24news\n",
    "annotation = 'C3M'       # 'C3M' ou 'our_annotation' ou 'combined_annotation'\n",
    "config = load_config(model_name, dataset)\n",
    "config.annotation = annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35cdb74e-0d6f-4e0c-bfb4-18a7d429ae21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des fichiers sauvegardés...\n"
     ]
    }
   ],
   "source": [
    "# # import the data\n",
    "from prepare_data import load_fc_prepare_data\n",
    "prepare_data = load_fc_prepare_data(config.DATASET)\n",
    "train_loader, test_loader, val_loader, train_df, val_df, test_df = prepare_data(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f964f317-66b6-4965-ae00-74592c7586cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Launching experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91ac0e8-3410-41d6-9c91-d7b52c896ec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import pipeline\n",
    "from concepts_discovery_utils import (\n",
    "    extract_target_words, create_context_window,\n",
    "    load_model, run_concepts_discovery, calculate_macro_concept_frequencies,\n",
    "    update_concept_frequencies, find_most_frequent_macro_concept,\n",
    "    \n",
    " ) #summarize_concepts, \n",
    "from attribution_utils import (\n",
    "    process_data_in_batches, get_example, example_attribution,\n",
    "    split_dataloader\n",
    ")\n",
    "import json\n",
    "\n",
    "# Ranking concepts \n",
    "from ranking_utils import rank_macro_concepts, most_k_important_macro_concepts, get_concept_at_rank, randomize_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a400d91-ab31-4637-a384-5ea397363aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Launch experimentation for OUR pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model_name == 'gemma':\n",
    "    from full_pipeline_gemma import JointResidualFittingModel\n",
    "    from models.jointCBMv2_gemma import JointModel\n",
    "else:\n",
    "    from full_pipeline import JointResidualFittingModel\n",
    "    from models.jointCBMv2 import JointModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9526898a-264e-4ee1-976d-b586bc804a38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Load the joint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b3f528-ec20-4b85-9a5b-44d8a64dde6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embedder_model, embedder_tokenizer, ModelXtoCtoY_layer, _ = load_model_and_tokenizer(config, n_concepts = 1)\n",
    "CBM_joint = JointModel(embedder_model, embedder_tokenizer, ModelXtoCtoY_layer, config, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223e2b5b-23a8-439a-a6b8-462bec186a33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.call of the residual fitting part for pipeline below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e68e65-b6d3-4ace-b6ec-57c5a25f4ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from models.utils import RidgeLinearLayer\n",
    "\n",
    "# residual fitting part for pipeline below\n",
    "linear_layer = RidgeLinearLayer(config.dim, config.num_labels, l2_lambda=0.01)\n",
    "linear_layer.to(config.device)\n",
    "\n",
    "CBM_joint.linear_layer = linear_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d4326d-91ea-4cbc-8aac-2fa068f584ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Instancie the pipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caafeefa-cfc5-4d2d-b443-ecdbc48dc0e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joint_residual_model = JointResidualFittingModel(\n",
    "    joint_model = CBM_joint,\n",
    "    linear_layer = linear_layer, \n",
    "    discovery_model = None, \n",
    "    discovery_tokenizer = None,\n",
    "    config = config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370f69f5-33f2-48ce-a3f8-918eb59853c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. launch the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  new dataset C3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import the dataframe\n",
    "from prepare_data import prepare_data_from_csv\n",
    "df_aug_train, df_aug_test = prepare_data_from_csv(annotation  = config.annotation, config = config, return_test = True)\n",
    "\n",
    "# # dataloaders\n",
    "from concepts_bank_utils import create_dataloader\n",
    "notre_loader_train = create_dataloader(df_aug_train, embedder_tokenizer, config.max_len, config.batch_size)\n",
    "notre_loader_test = create_dataloader(df_aug_test, embedder_tokenizer, config.max_len, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_heuristique_MJ_TCAVS\n",
      "[(['players'], 78.6), (['industry-specific terminology and jargon'], 97.575), (['companies'], 97.725), (['economic trends'], 98.0), (['industries'], 98.05), (['summaries of events'], 99.9), (['technological trends'], 99.975), (['financial markets'], 99.975), (['industry analysis'], 99.975), (['Scores'], 99.975), (['charts, graphs, and financial data'], 99.975), (['advancements in computing'], 99.975), (['stock market updates'], 99.975), (['innovations'], 99.975), (['emerging technologies'], 99.975), (['new products'], 99.975), (['software'], 99.975), (['hardware'], 99.975), (['political developments'], 99.975), (['global issues'], 99.975), (['international events'], 99.975), (['global politics'], 99.975), (['foreign affairs'], 99.975), (['international relations'], 99.975), (['sports events'], 99.975), (['athletic competitions'], 99.975), (['jargon specific to the sport'], 100.0), (['game summaries'], 100.0), (['teams'], 100.0), (['analysis of global implications'], 100.0), (['News about wars, conflicts'], 100.0), (['economic forecasts'], 100.0), (['player profiles'], 100.0), (['quotes from officials'], 100.0), (['company earnings reports'], 100.0), (['diplomacy'], 100.0), (['international organizations'], 100.0), (['interviews, and analysis of sports strategies'], 100.0), (['diplomatic language'], 100.0), (['research breakthroughs'], 100.0), (['diagrams, illustrations, and explanations of technical concepts'], 100.0)]\n",
      "\n",
      "--- Iteration 1 ---\n",
      "[Info] Complété pour atteindre au moins 12 concepts : ['players', 'industry-specific terminology and jargon', 'companies', 'economic trends', 'industries', 'summaries of events', 'technological trends', 'financial markets', 'industry analysis', 'Scores', 'charts, graphs, and financial data', 'advancements in computing']\n",
      "New concepts (itération 1): ['players', 'industry-specific terminology and jargon', 'companies', 'economic trends', 'industries', 'summaries of events', 'technological trends', 'financial markets', 'industry analysis', 'Scores', 'charts, graphs, and financial data', 'advancements in computing']\n",
      "Concepts discovered so far: ['players', 'industry-specific terminology and jargon', 'companies', 'economic trends', 'industries', 'summaries of events', 'technological trends', 'financial markets', 'industry analysis', 'Scores', 'charts, graphs, and financial data', 'advancements in computing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 14/250 [00:02<00:34,  6.89batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#'new_heuristique_MJ' (PS: use gemma model only with this strategy)  \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#'new_heuristique_LIG' or \u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# \"new_heuristique_MJ_by_lig_brute\" or \u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# \"new_heuristique_MJ_our_annotation\" or \u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# \"new_heuristique_MJ_combined_annotation\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# \"new_heuristique_MJ_TCAVS\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mjoint_residual_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_full_pipeline_tcavs_strategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnotre_loader_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnotre_loader_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs_residual_layer\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcavs_type_arg\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnew_heuristique_MJ_TCAVS\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoverage_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m95\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:122\u001b[39m, in \u001b[36mrun_full_pipeline_tcavs_strategy\u001b[39m\u001b[34m(self, train_loader, val_loader, test_loader, num_iterations, num_epochs_residual_layer, cavs_type_arg, strategy, coverage_threshold)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yann_CBM/Launch/notebooks/../run_experiments/models/jointCBMv2.py:132\u001b[39m, in \u001b[36mJointModel.train_model\u001b[39m\u001b[34m(self, train_loader, val_loader)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28mself\u001b[39m.embedder_model.train()\n\u001b[32m    130\u001b[39m \u001b[38;5;28mself\u001b[39m.ModelXtoCtoY_layer.train()\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTraining\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mXtoC_output\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yann_CBM/yann_env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yann_CBM/yann_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yann_CBM/yann_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    674\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    677\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yann_CBM/yann_env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     49\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yann_CBM/Launch/notebooks/../scripts/concepts_bank_utils.py:485\u001b[39m, in \u001b[36mCustomDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    481\u001b[39m label = \u001b[38;5;28mself\u001b[39m.labels[idx]\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# Création du dictionnaire des additional features\u001b[39;00m\n\u001b[32m    484\u001b[39m additional_features = {\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     key: torch.tensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdummy_col\u001b[49m\u001b[43m]\u001b[49m, dtype=torch.long)\n\u001b[32m    486\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, dummy_col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.additional_features_keys, \u001b[38;5;28mself\u001b[39m.additional_columns)\n\u001b[32m    487\u001b[39m }\n\u001b[32m    489\u001b[39m encoding = \u001b[38;5;28mself\u001b[39m.tokenizer.encode_plus(\n\u001b[32m    490\u001b[39m     text,\n\u001b[32m    491\u001b[39m     add_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    496\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    497\u001b[39m )\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# Fusionner les dictionnaires\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yann_CBM/yann_env/lib/python3.12/site-packages/pandas/core/series.py:1106\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m   1105\u001b[39m     check_dict_or_set_indexers(key)\n\u001b[32m-> \u001b[39m\u001b[32m1106\u001b[39m     key = \u001b[43mcom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_if_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mEllipsis\u001b[39m:\n\u001b[32m   1109\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write() \u001b[38;5;129;01mor\u001b[39;00m warn_copy_on_write():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Yann_CBM/yann_env/lib/python3.12/site-packages/pandas/core/common.py:372\u001b[39m, in \u001b[36mapply_if_callable\u001b[39m\u001b[34m(maybe_callable, obj, **kwargs)\u001b[39m\n\u001b[32m    365\u001b[39m     \u001b[38;5;66;03m# everything failed (probably because the argument\u001b[39;00m\n\u001b[32m    366\u001b[39m     \u001b[38;5;66;03m# wasn't actually callable); we return None\u001b[39;00m\n\u001b[32m    367\u001b[39m     \u001b[38;5;66;03m# instead of the empty string in this case to allow\u001b[39;00m\n\u001b[32m    368\u001b[39m     \u001b[38;5;66;03m# distinguishing between no name and a name of ''\u001b[39;00m\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_if_callable\u001b[39m(maybe_callable, obj, **kwargs):\n\u001b[32m    373\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[33;03m    Evaluate possibly callable input using obj and kwargs if it is callable,\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[33;03m    otherwise return as it is.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    381\u001b[39m \u001b[33;03m    **kwargs\u001b[39;00m\n\u001b[32m    382\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(maybe_callable):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#'new_heuristique_MJ' (PS: use gemma model only with this strategy)  \n",
    "#'new_heuristique_LIG' or \n",
    "# \"new_heuristique_MJ_by_lig_brute\" or \n",
    "# \"new_heuristique_MJ_our_annotation\" or \n",
    "# \"new_heuristique_MJ_combined_annotation\"\n",
    "# \"new_heuristique_MJ_TCAVS\"\n",
    "\n",
    "joint_residual_model.run_full_pipeline_tcavs_strategy(\n",
    "    train_loader=notre_loader_train,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=notre_loader_test, \n",
    "    num_iterations=10, \n",
    "    num_epochs_residual_layer=5,\n",
    "    cavs_type_arg = 'mean',\n",
    "    strategy = 'new_heuristique_MJ_TCAVS', \n",
    "    coverage_threshold = 95\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4_Joint_Incremental_CBM_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python (yann_kernel)",
   "language": "python",
   "name": "yann_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
