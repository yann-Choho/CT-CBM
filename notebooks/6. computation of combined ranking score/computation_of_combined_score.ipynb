{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0745a076-0599-4a14-a5ff-3e1b0ab4d383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ranking des concepts par LIG-score\n",
    "\n",
    "---\n",
    "Goal of the notebook: Ranker des concepts par LIG-score.\n",
    "\n",
    "Inputs of the notebook:\n",
    "-.\n",
    "Output of the notebook:\n",
    "\n",
    "-.\n",
    "Takeaways: \n",
    "- .\n",
    "- ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61184a1f-5298-493b-bf64-b31f6b8d93ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhan/.conda/envs/good_env/lib/python3.12/site-packages/sklearn/__init__.py:82: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.0)\n",
      "  import scipy.linalg  # noqa\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../run_experiments/')\n",
    "sys.path.append('../../run_experiments/scripts')\n",
    "sys.path.append('../../run_experiments/models')\n",
    "sys.path.append('../../run_experiments/data')\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# import fonction for getting PLM and tokenizer\n",
    "from models.utils import load_model_and_tokenizer\n",
    "\n",
    "# library for managing memory RAM\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import pickle, os, json, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db906d9c-47fa-440b-be84-b2bbb9e940cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f921208-c689-4c81-b68f-22c4922c67ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 0. autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50839d65-be00-4f76-8e12-c122b7b69d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#code for autoreload script associated with jupyter notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7110737a-9a5a-40fb-b645-5c9cd28f652e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1.SETUP\t ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6cae576-657f-403d-b4d4-e986af811158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import config\n",
    "from load_config import load_config\n",
    "\n",
    "model_name = 'bert-base-uncased'    # 'bert-base-uncased' ou 'deberta-large' or 'gemma'\n",
    "dataset    = 'agnews'               # 'movies' / 'agnews' / 'dbpedia' / 'medical'/ 'ledgar'/ n24news\n",
    "annotation = 'C3M'       # 'C3M' ou 'our_annotation' ou 'combined_annotation'\n",
    "config = load_config(model_name, dataset)\n",
    "config.annotation = annotation\n",
    "\n",
    "agg_mode = 'abs'\n",
    "agg_scope =\"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82f72545-1d9b-48bd-a777-d66f947d94fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Black Box Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d97f2f-7c8e-44a2-9cc3-69f1f3f7b290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from models.BaselineModel import BaselineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290b08e6-faa5-4cf9-87a9-5baad3d315b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhan/Yann_CBM/Launch/notebooks/../run_experiments/models/BaselineModel.py:199: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.classifier.load_state_dict(torch.load(f\"{self.save_path}blue_checkpoints/{self.config.model_name}/BaselineModel/{self.embedder_model_name}_classifier_state_dict.pth\"))\n",
      "/home/bhan/Yann_CBM/Launch/notebooks/../run_experiments/models/BaselineModel.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.embedder_model.load_state_dict(torch.load(f\"{self.save_path}blue_checkpoints/{self.config.model_name}/BaselineModel/{self.embedder_model_name}_state_dict.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucune performance enregistrée trouvée.\n",
      "Tous les paramètres sont maintenant en float64 : True\n"
     ]
    }
   ],
   "source": [
    "# import the PLM model and tokenizer and bottleneck layer\n",
    "embedder_model, embedder_tokenizer, _, classifier = load_model_and_tokenizer(config, n_concepts = 4)\n",
    "\n",
    "black_box_model = BaselineModel(embedder_model, classifier, None, None, None, config, save_path = config.SAVE_PATH)\n",
    "black_box_model.load_model()\n",
    "\n",
    "# Fonction pour vérifier si les paramètres du modèle sont en float64\n",
    "def check_parameters_dtype(model):\n",
    "    for param in model.parameters():\n",
    "        if param.dtype != torch.float64:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "# Fonction pour convertir les paramètres du modèle en float64\n",
    "def convert_parameters_to_float64(model):\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.double()\n",
    "\n",
    "# Supposons que `black_box_model` est l'instance du modèle\n",
    "convert_parameters_to_float64(black_box_model)\n",
    "\n",
    "# Vérifier si tous les paramètres sont maintenant en float64\n",
    "is_float64 = check_parameters_dtype(black_box_model)\n",
    "\n",
    "print(f\"Tous les paramètres sont maintenant en float64 : {is_float64}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e490e62d-dd03-44aa-9718-45e414df444e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.0 create dataloader with augmented concepts bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import the dataframe\n",
    "from prepare_data import prepare_data_from_csv\n",
    "df_aug_train, df_aug_test = prepare_data_from_csv(annotation  = config.annotation, config = config, return_test = True)\n",
    "\n",
    "# dataloaders\n",
    "from concepts_bank_utils import create_dataloader\n",
    "notre_loader_train = create_dataloader(df_aug_train, embedder_tokenizer, config.max_len, config.batch_size)\n",
    "notre_loader_test = create_dataloader(df_aug_test, embedder_tokenizer, config.max_len, config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des vecteurs CAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if annotation == 'C3M' :\n",
    "    file_path = f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs/{config.cavs_type}/cavs_{config.cavs_type}_C3M.json\"\n",
    "elif annotation == 'our_annotation':\n",
    "    file_path = f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs/{config.cavs_type}/cavs_{config.cavs_type}_our_annotation.json\"\n",
    "else:\n",
    "    print(f'no cavs computed for annotation : {annotation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cavs chargés à partir de /home/bhan/Yann_CBM/Launch/dbfs/results_agnews//blue_checkpoints/bert-base-uncased/cavs/mean/cavs_mean_C3M.json\n"
     ]
    }
   ],
   "source": [
    "# Chargement des vecteurs CAV à partir d'un fichier JSON et conversion sur GPU\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        cavs_vectors = json.load(f)\n",
    "    cavs = {k: torch.tensor(v, dtype=torch.float32).to(config.device) for k, v in cavs_vectors.items()}\n",
    "    print(\"cavs chargés à partir de\", file_path)\n",
    "else:\n",
    "    print(\"Fichier cavs introuvable :\", file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importer les vectors CAVS selon le type \"mean\" ou \"svm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Calculer le concept identifiability score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model_name =='gemma':\n",
    "    from new_heuristique import compute_cosine_matrix_and_metrics_gemma_version as compute_cosine_matrix_and_metrics\n",
    "else:\n",
    "    from new_heuristique import compute_cosine_matrix_and_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concept identifiability score = F1 score de detection sur le validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prendre le threshold en se basant sur la mediane cosine similarity sur le train set (pour l'utiliser comme threshold sur le test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de cosine_df depuis /home/bhan/Yann_CBM/Launch/dbfs/results_agnews//blue_checkpoints/bert-base-uncased/cavs/mean/cosine_df_C3M.pkl\n"
     ]
    }
   ],
   "source": [
    "# cosine_df, metrics, well_detected_concepts = compute_cosine_matrix_and_metrics(\n",
    "cosine_train, cosine_val, cosine_df, thresholds, metrics, well_detected_concepts = compute_cosine_matrix_and_metrics(\n",
    "    df=df_aug_train,\n",
    "    text_column=\"text\",\n",
    "    embedder_model=embedder_model,\n",
    "    embedder_tokenizer=embedder_tokenizer,\n",
    "    cavs=cavs,\n",
    "    f1_cutoff=0.1,  # Par exemple, ne retenir que les concepts dont le F1 >= 0.5\n",
    "    device=config.device,  # ex: torch.device(\"cuda\")\n",
    "    save_dir=f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs/{config.cavs_type}\",\n",
    "    config = config,\n",
    "    annotation = annotation,\n",
    "    cos_cubed = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_heuristique import (clean_concept_name, rename_dataframe_columns, \n",
    "filter_concepts_by_coverage, \n",
    "filter_concepts_by_coverage,\n",
    "order_well_detected_concepts )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Compute the Global indice for ranking concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 load all the 3 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequencies or presence_scores\n",
    "presence_scores = {\n",
    "    clean_concept_name(concept): df_aug_train[concept].mean()\n",
    "    for concept in df_aug_train.columns\n",
    "    if concept not in ['text','label','Unnamed: 0']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier /home/bhan/Yann_CBM/Launch/dbfs/results_agnews//blue_checkpoints/bert-base-uncased/cavs/mean/detection_concept_mean_C3M_abs_all.json déjà présent, chargement...\n"
     ]
    }
   ],
   "source": [
    "# concept identifiability score\n",
    "file_path_1 = (\n",
    "f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs/{config.cavs_type}/\"\n",
    "f\"detection_concept_{config.cavs_type}_{config.annotation}_{config.agg_mode}_{config.agg_scope}.json\"\n",
    ")\n",
    "\n",
    "\n",
    "if os.path.exists(file_path_1):\n",
    "    with open(file_path_1, \"r\") as f:\n",
    "        well_detected_concepts = json.load(f)\n",
    "    print(f\"Fichier {file_path_1} déjà présent, chargement...\")\n",
    "well_detected_concepts = {key: perf['F1'] for key, perf in well_detected_concepts.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier /home/bhan/Yann_CBM/Launch/dbfs/results_agnews//blue_checkpoints/bert-base-uncased/cavs/mean/sorted_macro_concepts_C3M.json déjà présent, chargement...\n"
     ]
    }
   ],
   "source": [
    "### tcavs\n",
    "file_path_2 = (\n",
    "    f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs/{config.cavs_type}/\"\n",
    "    f\"sorted_macro_concepts_C3M.json\"\n",
    ")\n",
    "if os.path.exists(file_path_2):\n",
    "    with open(file_path_2, \"r\") as f:\n",
    "        all_concepts_sorted_by_tcavs = json.load(f)\n",
    "    print(f\"Fichier {file_path_2} déjà présent, chargement...\")\n",
    "all_concepts_sorted_by_tcavs = { tuple[0]: tuple[1] for tuple in all_concepts_sorted_by_tcavs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIG Ranking \n",
    "# --- Chargement et préparation des concepts depuis le fichier JSON ---\n",
    "file_path_3 = (\n",
    "    f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs/\"\n",
    "    f\"{config.cavs_type}/sorted_macro_concepts_cosine_sm_{config.cavs_type}_{config.annotation}_{config.agg_mode}_{config.agg_scope}.json\"\n",
    "    )\n",
    "# Charger le fichier JSON\n",
    "with open(file_path_3, \"r\") as f:\n",
    "    sorted_macro_concepts_cosine_sm = json.load(f)\n",
    "\n",
    "# Supprimer les doublons\n",
    "sorted_macro_concepts_cosine_sm = list(dict.fromkeys(tuple(item) for item in sorted_macro_concepts_cosine_sm))\n",
    "\n",
    "# Appliquer le nettoyage sur la liste chargée\n",
    "all_concepts_sorted_by_lig = [(clean_concept_name(name), score) for name, score in sorted_macro_concepts_cosine_sm]\n",
    "all_concepts_sorted_by_lig = { tuple[0]: tuple[1] for tuple in all_concepts_sorted_by_lig}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Exemple de données ; à adapter avec vos variables\n",
    "# presence_scores, well_detected_concepts, all_concepts_sorted_by_tcavs et all_concepts_sorted_by_lig\n",
    "\n",
    "plot_df = pd.DataFrame({\n",
    "    \"Presence (Mean)\": presence_scores,\n",
    "    \"F1 Score detected\": well_detected_concepts,\n",
    "    \"TCAVS score\": all_concepts_sorted_by_tcavs,\n",
    "    \"LIG score\": all_concepts_sorted_by_lig\n",
    "})\n",
    "\n",
    "print(\"Shape du DataFrame initial :\", plot_df.shape)\n",
    "\n",
    "# Normalisation min-max pour les scores LIG \n",
    "#(don't automatically normalise TCAVS score: don't do it if blackboxe model is just a linear layer wwithout relu or sigmoid activation function) \n",
    "# plot_df[\"TCAVS score norm\"] = (plot_df[\"TCAVS score\"] - plot_df[\"TCAVS score\"].min()) / (plot_df[\"TCAVS score\"].max() - plot_df[\"TCAVS score\"].min())\n",
    "plot_df[\"LIG score norm\"] = (plot_df[\"LIG score\"] - plot_df[\"LIG score\"].min()) / (plot_df[\"LIG score\"].max() - plot_df[\"LIG score\"].min())\n",
    "\n",
    "\n",
    "plot_df[\"combined_score_TCAVS\"] =  plot_df[\"F1 Score detected\"] * plot_df[\"TCAVS score\"]\n",
    "plot_df[\"combined_score_LIG\"] = plot_df[\"F1 Score detected\"] * plot_df[\"LIG score norm\"]\n",
    "\n",
    "# Calcul des scores combinés en utilisant les scores normalisés\n",
    "# plot_df[\"combined_score_TCAVS\"] = plot_df[\"Presence (Mean)\"] * plot_df[\"F1 Score detected\"] * plot_df[\"TCAVS score\"]\n",
    "# plot_df[\"combined_score_LIG\"] = plot_df[\"Presence (Mean)\"] * plot_df[\"F1 Score detected\"] * plot_df[\"LIG score norm\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save the score for ulterior usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape du DataFrame initial : (41, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Presence (Mean)</th>\n",
       "      <th>F1 Score detected</th>\n",
       "      <th>TCAVS score</th>\n",
       "      <th>LIG score</th>\n",
       "      <th>LIG score norm</th>\n",
       "      <th>combined_score_TCAVS</th>\n",
       "      <th>combined_score_LIG</th>\n",
       "      <th>concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>international events</th>\n",
       "      <td>0.46525</td>\n",
       "      <td>0.757895</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.029520</td>\n",
       "      <td>0.806204</td>\n",
       "      <td>0.189474</td>\n",
       "      <td>0.611018</td>\n",
       "      <td>international events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global politics</th>\n",
       "      <td>0.31275</td>\n",
       "      <td>0.719524</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.021578</td>\n",
       "      <td>0.149151</td>\n",
       "      <td>0.179881</td>\n",
       "      <td>0.107318</td>\n",
       "      <td>global politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>international relations</th>\n",
       "      <td>0.29100</td>\n",
       "      <td>0.688822</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.021659</td>\n",
       "      <td>0.155840</td>\n",
       "      <td>0.172205</td>\n",
       "      <td>0.107346</td>\n",
       "      <td>international relations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foreign affairs</th>\n",
       "      <td>0.29325</td>\n",
       "      <td>0.710448</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.021693</td>\n",
       "      <td>0.158712</td>\n",
       "      <td>0.177612</td>\n",
       "      <td>0.112757</td>\n",
       "      <td>foreign affairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>News about wars, conflicts</th>\n",
       "      <td>0.19500</td>\n",
       "      <td>0.512224</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.021624</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.128056</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>News about wars, conflicts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Presence (Mean)  F1 Score detected  TCAVS score  \\\n",
       "international events                0.46525           0.757895         0.25   \n",
       "global politics                     0.31275           0.719524         0.25   \n",
       "international relations             0.29100           0.688822         0.25   \n",
       "foreign affairs                     0.29325           0.710448         0.25   \n",
       "News about wars, conflicts          0.19500           0.512224         0.25   \n",
       "\n",
       "                            LIG score  LIG score norm  combined_score_TCAVS  \\\n",
       "international events         0.029520        0.806204              0.189474   \n",
       "global politics              0.021578        0.149151              0.179881   \n",
       "international relations      0.021659        0.155840              0.172205   \n",
       "foreign affairs              0.021693        0.158712              0.177612   \n",
       "News about wars, conflicts   0.021624        0.153000              0.128056   \n",
       "\n",
       "                            combined_score_LIG                     concept  \n",
       "international events                  0.611018        international events  \n",
       "global politics                       0.107318             global politics  \n",
       "international relations               0.107346     international relations  \n",
       "foreign affairs                       0.112757             foreign affairs  \n",
       "News about wars, conflicts            0.078370  News about wars, conflicts  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# save the score\n",
    "path_plot_df = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/combined_score_concept_{config.annotation}_{config.agg_mode}_{config.agg_scope}.csv\"\n",
    "    )\n",
    "plot_df['concept'] = plot_df.index\n",
    "plot_df.to_csv(path_plot_df, index = False)\n",
    "plot_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_plot_df = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/combined_score_concept_{config.annotation}_{config.agg_mode}_{config.agg_scope}.csv\"\n",
    "    )\n",
    "plot_df = pd.read_csv(path_plot_df)\n",
    "\n",
    "df_sorted_tcavs = plot_df.sort_values(by ='combined_score_TCAVS', ascending = False)\n",
    "df_sorted_lig = plot_df.sort_values(by ='combined_score_LIG', ascending = False)\n",
    "combined_score_TCAVS = dict(zip(df_sorted_tcavs['concept'], df_sorted_tcavs['combined_score_TCAVS']))\n",
    "combined_score_LIG = dict(zip(df_sorted_lig['concept'], df_sorted_lig['combined_score_LIG']))\n",
    "\n",
    "path_combined_score_LIG = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/combined_score_LIG_{config.annotation}_{config.agg_mode}_{config.agg_scope}.json\"\n",
    "    )\n",
    "\n",
    "path_combined_score_TCAVS = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/combined_score_TCAVS_{config.annotation}_{config.agg_mode}_{config.agg_scope}.json\"\n",
    "    )\n",
    "\n",
    "with open(path_combined_score_LIG, 'w') as f:\n",
    "    json.dump(combined_score_LIG, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(path_combined_score_TCAVS, 'w') as f:\n",
    "    json.dump(combined_score_TCAVS, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BY TCAVS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Partie 2 : Récupération de la liste ordonnée des concepts et calcul du coverage cumulatif ---\n",
    "\n",
    "plot_df = plot_df.set_index('concept', drop=False)\n",
    "ordered_concepts = plot_df.sort_values(by=\"combined_score_TCAVS\", ascending=False).index.tolist()\n",
    "\n",
    "# Fonction de nettoyage identique à celle utilisée pour les concepts\n",
    "def clean_concept_name(name):\n",
    "    name = name.replace(\"cos_\", \"\").replace(\"dummy_\", \"\").strip()\n",
    "    name = re.sub(r\"Let me know.*\", \"\", name)\n",
    "    return \" \".join(name.split())\n",
    "\n",
    "# On s'assure que les noms de colonnes du DataFrame sont nettoyés pour correspondre aux noms présents dans ordered_concepts\n",
    "df_aug_train.rename(columns=lambda x: clean_concept_name(x), inplace=True)\n",
    "\n",
    "# --- Calcul de la courbe cumulative à partir de df_aug_train ---\n",
    "counts = []\n",
    "concepts_iter = []\n",
    "for concept in ordered_concepts:\n",
    "    # Ajout du concept courant dans la liste cumulée\n",
    "    concepts_iter.append(concept)\n",
    "    filtered_data = df_aug_train[concepts_iter][df_aug_train[concepts_iter].eq(1).any(axis=1)]\n",
    "    counts.append(len(filtered_data))\n",
    "\n",
    "# Calcul du pourcentage de lignes couvertes par rapport au total du DataFrame\n",
    "percentage = [100 * c / df_aug_train.shape[0] for c in counts]\n",
    "\n",
    "# Positions pour l'axe des x (une par concept)\n",
    "positions = np.arange(len(ordered_concepts))\n",
    "\n",
    "# --- Affichage du graphique combiné ---\n",
    "fig, ax1 = plt.subplots(figsize=(18, 15))\n",
    "\n",
    "values = plot_df.loc[ordered_concepts, \"combined_score_TCAVS\"]  # ou \"LIG score\", selon le choix\n",
    "\n",
    "ax1.bar(positions, values, color='skyblue', alpha=0.7, label=\"Valeur moyenne des concepts\")\n",
    "ax1.set_ylabel(\"combined_score_TCAVS\")\n",
    "ax1.set_xlabel(\"Concepts\")\n",
    "ax1.set_title(\"Valeur moyenne des concepts et couverture cumulative\")\n",
    "ax1.set_xticks(positions)\n",
    "ax1.set_xticklabels(ordered_concepts, rotation=45, ha='right')\n",
    "ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Axe secondaire : Courbe cumulative (pourcentage de lignes couvertes)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(positions, percentage, marker='o', color='darkorange', label=\"Couverture cumulative\")\n",
    "ax2.set_ylabel(\"Pourcentage de lignes avec au moins un concept (%)\", color='darkorange')\n",
    "ax2.tick_params(axis='y', colors='darkorange')\n",
    "ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Légendes combinées\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_coverage = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/sorted_macro_concepts_coverage_{config.annotation}_{config.agg_mode}_{config.agg_scope}_TCAVS.json\"\n",
    "    )\n",
    "\n",
    "score_dict = dict(zip(ordered_concepts, percentage))\n",
    "\n",
    "with open(path_coverage, 'w') as f:\n",
    "    json.dump(score_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics d'aggregation par cluster\n",
    "\n",
    "path_plot_df = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/combined_score_concept_{config.annotation}_{config.agg_mode}_{config.agg_scope}.csv\"\n",
    "    )\n",
    "\n",
    "plot_df = pd.read_csv(path_plot_df)\n",
    "# plot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cluster_dict_strat_reassignation = f\"{config.SAVE_PATH_CONCEPTS}/cluster_dict_strat_reassignation_C3M.json\"\n",
    "\n",
    "path_cluster_dict_strat_noise_cluster = f\"{config.SAVE_PATH_CONCEPTS}/cluster_dict_strat_noise_cluster_C3M.json\"\n",
    "                                    \n",
    "with open(path_cluster_dict_strat_reassignation, 'r') as f:\n",
    "   cluster_dict_strat_reassignation = json.load(f)\n",
    "\n",
    "with open(path_cluster_dict_strat_noise_cluster, 'r') as f:\n",
    "    cluster_dict_strat_noise_cluster = json.load(f)\n",
    "\n",
    "\n",
    "def new_columns_reassignation(concept):\n",
    "    for k, v in cluster_dict_strat_reassignation.items():\n",
    "        if concept in v:\n",
    "            return int(k)\n",
    "    return -1  # ou autre valeur par défaut si pas trouvé\n",
    "\n",
    "def new_columns_noisy(concept):\n",
    "    for k, v in path_cluster_dict_strat_noise_cluster.items():\n",
    "        if concept in v:\n",
    "            return int(k)\n",
    "    return -1  # ou autre valeur par défaut si pas trouvé\n",
    "\n",
    "plot_df['cluster'] = plot_df['concept'].apply(new_columns_reassignation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import hdbscan\n",
    "from umap import UMAP\n",
    "\n",
    "# 0) On met le nom du concept en index\n",
    "plot_df = plot_df.set_index(\"concept\")\n",
    "\n",
    "# 4) Construction : cluster → concepts triés par score\n",
    "cluster_groups = {}\n",
    "for c in sorted(set(set(plot_df['cluster'].to_list()))):\n",
    "    lst = (plot_df\n",
    "           .loc[plot_df[\"cluster\"] == c]\n",
    "           .sort_values(\"combined_score_TCAVS\", ascending=False)\n",
    "           .index\n",
    "           .tolist())\n",
    "    cluster_groups[c] = lst\n",
    "\n",
    "# 5) Sélection round-robin + suivi du coverage par étape\n",
    "total = df_aug_train.shape[0]\n",
    "working = {c: cluster_groups[c].copy() for c in cluster_groups}\n",
    "\n",
    "# Stockage des résultats\n",
    "coverage_evolution = []\n",
    "selected_concepts = []\n",
    "\n",
    "while any(working[c] for c in working):\n",
    "    current_selection = []\n",
    "    for c in sorted(working):\n",
    "        if working[c]:\n",
    "            concept = working[c].pop(0)\n",
    "            current_selection.append(concept)\n",
    "            selected_concepts.append(concept)\n",
    "    \n",
    "    # Calcul du coverage après avoir ajouté ces concepts\n",
    "    mask = df_aug_train[selected_concepts].eq(1).any(axis=1)\n",
    "    coverage = mask.sum() * 100 / total\n",
    "    \n",
    "    coverage_evolution.append((current_selection.copy(), coverage))\n",
    "\n",
    "# 6) Affichage\n",
    "for idx, (concepts, cov) in enumerate(coverage_evolution):\n",
    "    print(f\"Step {idx+1}: Concepts {concepts} → Coverage {cov:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cumuler le nombre total de concepts sélectionnés à chaque étape\n",
    "n_concepts_selected = np.cumsum([len(concepts) for concepts, _ in coverage_evolution])\n",
    "coverage_values = [cov for _, cov in coverage_evolution]\n",
    "\n",
    "# Tracé\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(n_concepts_selected, coverage_values, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Nombre de concepts sélectionnés\")\n",
    "plt.ylabel(\"Coverage cumulé (%)\")\n",
    "plt.title(\"Évolution du coverage en sélection round-robin par cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path_coverage = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/sorted_macro_concepts_coverage_MJ_{config.annotation}_{config.agg_mode}_{config.agg_scope}_TCAVS.pkl\"\n",
    "    )\n",
    "# Sauvegarde\n",
    "with open(path_coverage, \"wb\") as f:\n",
    "    pickle.dump(coverage_evolution, f)\n",
    "\n",
    "print(f\"✅ Fichier {path_coverage} en .pkl' sauvegardé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_coverage, \"rb\") as f:\n",
    "    loaded_coverage_evolution_ = pickle.load(f)\n",
    "loaded_coverage_evolution_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BY LIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics d'aggregation par cluster\n",
    "\n",
    "path_plot_df = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/combined_score_concept_{config.annotation}_{config.agg_mode}_{config.agg_scope}.csv\"\n",
    "    )\n",
    "\n",
    "plot_df = pd.read_csv(path_plot_df)\n",
    "# plot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# --- Partie 2 : Récupération de la liste ordonnée des concepts et calcul du coverage cumulatif ---\n",
    "\n",
    "# Ici, nous choisissons par exemple de trier les concepts selon le combined_score_TCAVS décroissant.\n",
    "# La même démarche peut être faite avec combined_score_LIG\n",
    "\n",
    "plot_df = plot_df.set_index('concept', drop=False)\n",
    "# ordered_concepts = plot_df.sort_values(by=\"combined_score_LIG\", ascending=False).to_list()\n",
    "ordered_concepts = plot_df.sort_values(by=\"combined_score_LIG\", ascending=False)['concept'].to_list()\n",
    "\n",
    "print(\"\\nListe des concepts triée par combined_score_TCAVS (du plus grand au plus petit) :\")\n",
    "print(ordered_concepts)\n",
    "\n",
    "\n",
    "# --- Chargement du DataFrame de données réelles (df_aug_train) et préparation des colonnes ---\n",
    "# Exemple de chargement : on suppose que df_aug_train contient une colonne pour chacun des concepts,\n",
    "# avec 1 indiquant la présence du concept dans la ligne.\n",
    "# Attention : vous devez adapter le chemin et le nettoyage des colonnes afin que les noms correspondent.\n",
    "\n",
    "# Fonction de nettoyage identique à celle utilisée pour les concepts\n",
    "def clean_concept_name(name):\n",
    "    name = name.replace(\"cos_\", \"\").replace(\"dummy_\", \"\").strip()\n",
    "    name = re.sub(r\"Let me know.*\", \"\", name)\n",
    "    return \" \".join(name.split())\n",
    "\n",
    "# On s'assure que les noms de colonnes du DataFrame sont nettoyés pour correspondre aux noms présents dans ordered_concepts\n",
    "df_aug_train.rename(columns=lambda x: clean_concept_name(x), inplace=True)\n",
    "\n",
    "# --- Calcul de la courbe cumulative à partir de df_aug_train ---\n",
    "counts = []\n",
    "concepts_iter = []\n",
    "for concept in ordered_concepts:\n",
    "    # Ajout du concept courant dans la liste cumulée\n",
    "    concepts_iter.append(concept)\n",
    "    # Sélection des lignes où au moins un des concepts cumulés est présent (valeur == 1)\n",
    "    # On suppose que dans df_aug_train, pour chaque concept, la présence est indiquée par 1 (sinon adapter)\n",
    "    filtered_data = df_aug_train[concepts_iter][df_aug_train[concepts_iter].eq(1).any(axis=1)]\n",
    "    counts.append(len(filtered_data))\n",
    "\n",
    "# Calcul du pourcentage de lignes couvertes par rapport au total du DataFrame\n",
    "percentage = [100 * c / df_aug_train.shape[0] for c in counts]\n",
    "\n",
    "# Positions pour l'axe des x (une par concept)\n",
    "positions = np.arange(len(ordered_concepts))\n",
    "\n",
    "# --- Affichage du graphique combiné ---\n",
    "fig, ax1 = plt.subplots(figsize=(18, 15))\n",
    "\n",
    "# Graphique 1 : Barres verticales pour la valeur moyenne (ici, on peut choisir d'afficher la valeur du score associé du JSON)\n",
    "# Dans cet exemple, on affiche les scores moyens provenant du JSON (extrait du plot_df précédemment)\n",
    "# Si nécessaire, vous pouvez adapter la barre pour afficher une autre valeur.\n",
    "values = plot_df.loc[ordered_concepts, \"combined_score_LIG\"]  # ou \"LIG score\", selon le choix\n",
    "\n",
    "ax1.bar(positions, values, color='skyblue', alpha=0.7, label=\"Valeur moyenne des concepts\")\n",
    "ax1.set_ylabel(\"combined_score_LIG\")\n",
    "ax1.set_xlabel(\"Concepts\")\n",
    "ax1.set_title(\"Valeur moyenne des concepts et couverture cumulative\")\n",
    "ax1.set_xticks(positions)\n",
    "ax1.set_xticklabels(ordered_concepts, rotation=45, ha='right')\n",
    "ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Axe secondaire : Courbe cumulative (pourcentage de lignes couvertes)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(positions, percentage, marker='o', color='darkorange', label=\"Couverture cumulative\")\n",
    "ax2.set_ylabel(\"Pourcentage de lignes avec au moins un concept (%)\", color='darkorange')\n",
    "ax2.tick_params(axis='y', colors='darkorange')\n",
    "ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Légendes combinées\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_coverage = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/sorted_macro_concepts_coverage_{config.annotation}_{config.agg_mode}_{config.agg_scope}.json\"\n",
    "    )\n",
    "\n",
    "score_dict = dict(zip(ordered_concepts, percentage))\n",
    "\n",
    "with open(path_coverage, 'w') as f:\n",
    "    json.dump(score_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Metrics d'aggregation par cluster\n",
    "\n",
    "path_plot_df = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/combined_score_concept_{config.annotation}_{config.agg_mode}_{config.agg_scope}.csv\"\n",
    "    )\n",
    "\n",
    "plot_df = pd.read_csv(path_plot_df)\n",
    "# plot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cluster_dict_strat_reassignation = f\"{config.SAVE_PATH_CONCEPTS}/cluster_dict_strat_reassignation_C3M.json\"\n",
    "\n",
    "path_cluster_dict_strat_noise_cluster = f\"{config.SAVE_PATH_CONCEPTS}/cluster_dict_strat_noise_cluster_C3M.json\"\n",
    "                                    \n",
    "with open(path_cluster_dict_strat_reassignation, 'r') as f:\n",
    "   cluster_dict_strat_reassignation = json.load(f)\n",
    "\n",
    "with open(path_cluster_dict_strat_noise_cluster, 'r') as f:\n",
    "    cluster_dict_strat_noise_cluster = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_columns_reassignation(concept):\n",
    "    for k, v in cluster_dict_strat_reassignation.items():\n",
    "        if concept in v:\n",
    "            return int(k)\n",
    "    return -1  # ou autre valeur par défaut si pas trouvé\n",
    "\n",
    "def new_columns_noisy(concept):\n",
    "    for k, v in path_cluster_dict_strat_noise_cluster.items():\n",
    "        if concept in v:\n",
    "            return int(k)\n",
    "    return -1  # ou autre valeur par défaut si pas trouvé\n",
    "\n",
    "plot_df['cluster'] = plot_df['concept'].apply(new_columns_reassignation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import hdbscan\n",
    "from umap import UMAP\n",
    "\n",
    "# 0) On met le nom du concept en index\n",
    "plot_df = plot_df.set_index(\"concept\")\n",
    "\n",
    "# 4) Construction : cluster → concepts triés par score\n",
    "cluster_groups = {}\n",
    "for c in sorted(set(set(plot_df['cluster'].to_list()))):\n",
    "    lst = (plot_df\n",
    "           .loc[plot_df[\"cluster\"] == c]\n",
    "           .sort_values(\"combined_score_TCAVS\", ascending=False)\n",
    "           .index\n",
    "           .tolist())\n",
    "    cluster_groups[c] = lst\n",
    "\n",
    "# 5) Sélection round-robin + suivi du coverage par étape\n",
    "total = df_aug_train.shape[0]\n",
    "working = {c: cluster_groups[c].copy() for c in cluster_groups}\n",
    "\n",
    "# Stockage des résultats\n",
    "coverage_evolution = []\n",
    "selected_concepts = []\n",
    "\n",
    "while any(working[c] for c in working):\n",
    "    current_selection = []\n",
    "    for c in sorted(working):\n",
    "        if working[c]:\n",
    "            concept = working[c].pop(0)\n",
    "            current_selection.append(concept)\n",
    "            selected_concepts.append(concept)\n",
    "    \n",
    "    # Calcul du coverage après avoir ajouté ces concepts\n",
    "    mask = df_aug_train[selected_concepts].eq(1).any(axis=1)\n",
    "    coverage = mask.sum() * 100 / total\n",
    "    \n",
    "    coverage_evolution.append((current_selection.copy(), coverage))\n",
    "\n",
    "# 6) Affichage\n",
    "for idx, (concepts, cov) in enumerate(coverage_evolution):\n",
    "    print(f\"Step {idx+1}: Concepts {concepts} → Coverage {cov:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cumuler le nombre total de concepts sélectionnés à chaque étape\n",
    "n_concepts_selected = np.cumsum([len(concepts) for concepts, _ in coverage_evolution])\n",
    "coverage_values = [cov for _, cov in coverage_evolution]\n",
    "\n",
    "# Tracé\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(n_concepts_selected, coverage_values, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Nombre de concepts sélectionnés\")\n",
    "plt.ylabel(\"Coverage cumulé (%)\")\n",
    "plt.title(\"Évolution du coverage en sélection round-robin par cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path_coverage = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/sorted_macro_concepts_coverage_MJ_{config.annotation}_{config.agg_mode}_{config.agg_scope}.pkl\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Sauvegarde\n",
    "with open(path_coverage, \"wb\") as f:\n",
    "    pickle.dump(coverage_evolution, f)\n",
    "\n",
    "print(\"✅ Fichier 'coverage_evolution.pkl' sauvegardé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_coverage, \"rb\") as f:\n",
    "    loaded_coverage_evolution = pickle.load(f)\n",
    "loaded_coverage_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abblation study on 10 random generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics d'aggregation par cluster\n",
    "\n",
    "path_plot_df = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/combined_score_concept_{config.annotation}_{config.agg_mode}_{config.agg_scope}.csv\"\n",
    "    )\n",
    "\n",
    "plot_df = pd.read_csv(path_plot_df)\n",
    "# plot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# On reprend les mêmes df et dicts que précédemment :\n",
    "# - plot_df : DataFrame chargé depuis combined_score_concept_…csv\n",
    "# - df_aug_train : ton DataFrame d'entraînement binaire (colonnes « concept » = 1 ou 0)\n",
    "# - cluster_dict_strat_reassignation et cluster_dict_strat_noise_cluster déjà chargés\n",
    "\n",
    "# Chemins de base\n",
    "path_plot_df = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/combined_score_concept_{config.annotation}_{config.agg_mode}_{config.agg_scope}.csv\"\n",
    "    )\n",
    "\n",
    "# Préparer les clusters\n",
    "with open(f\"{config.SAVE_PATH_CONCEPTS}/cluster_dict_strat_reassignation_C3M.json\", \"r\") as f:\n",
    "    cluster_reassign = json.load(f)\n",
    "with open(f\"{config.SAVE_PATH_CONCEPTS}/cluster_dict_strat_noise_cluster_C3M.json\", \"r\") as f:\n",
    "    cluster_noise = json.load(f)\n",
    "\n",
    "def assign_cluster(concept, dct):\n",
    "    for k,v in dct.items():\n",
    "        if concept in v:\n",
    "            return int(k)\n",
    "    return -1\n",
    "\n",
    "plot_df[\"cluster\"] = plot_df.index.map(lambda c: assign_cluster(c, cluster_reassign))\n",
    "\n",
    "# Nombre total de lignes pourcentage\n",
    "total = df_aug_train.shape[0]\n",
    "\n",
    "for run in range(10):\n",
    "    # 1) Flipper / mélanger les scores LIG\n",
    "    shuffled = plot_df[\"combined_score_LIG\"].sample(frac=1, random_state=run).values\n",
    "    plot_df[\"combined_score_LIG_shuffled\"] = shuffled\n",
    "    \n",
    "    # 2) Construire les groupes par cluster, triés par score mélangé\n",
    "    cluster_groups = {\n",
    "        c: plot_df.loc[plot_df.cluster == c]\n",
    "                  .sort_values(\"combined_score_LIG_shuffled\", ascending=False)\n",
    "                  .index\n",
    "                  .tolist()\n",
    "        for c in plot_df[\"cluster\"].unique()\n",
    "    }\n",
    "    working = {c: lst.copy() for c,lst in cluster_groups.items()}\n",
    "\n",
    "    # 3) Round‑robin et calcul du coverage cumulatif\n",
    "    selected = []\n",
    "    coverage_evolution = []\n",
    "    while any(working[c] for c in working):\n",
    "        batch = []\n",
    "        for c in sorted(working):\n",
    "            if working[c]:\n",
    "                batch.append(working[c].pop(0))\n",
    "                selected.append(batch[-1])\n",
    "        # coverage après ajout de batch\n",
    "        mask = df_aug_train[selected].eq(1).any(axis=1)\n",
    "        coverage = mask.sum() * 100.0 / total\n",
    "        coverage_evolution.append((batch.copy(), coverage))\n",
    "\n",
    "    # 4) Sauvegarde du .pkl pour ce run\n",
    "    out_path = base_cavs / f\"sorted_macro_concepts_coverage_MJ_{config.annotation}_{config.agg_mode}_{config.agg_scope}_{run}.pkl\"\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(coverage_evolution, f)\n",
    "    print(f\"✅ run {run} → {out_path.name} ({len(coverage_evolution)} étapes) sauvegardé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abblation study (SCORE = LIG) without F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics d'aggregation par cluster\n",
    "\n",
    "path_plot_df = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/combined_score_concept_{config.annotation}_{config.agg_mode}_{config.agg_scope}.csv\"\n",
    "    )\n",
    "\n",
    "plot_df = pd.read_csv(path_plot_df)\n",
    "# plot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# --- Partie 2 : Récupération de la liste ordonnée des concepts et calcul du coverage cumulatif ---\n",
    "\n",
    "# Ici, nous choisissons par exemple de trier les concepts selon le combined_score_TCAVS décroissant.\n",
    "# La même démarche peut être faite avec combined_score_LIG\n",
    "\n",
    "plot_df = plot_df.set_index('concept', drop=False)\n",
    "# ordered_concepts = plot_df.sort_values(by=\"combined_score_LIG\", ascending=False).to_list()\n",
    "ordered_concepts = plot_df.sort_values(by=\"LIG score\", ascending=False)['concept'].to_list()\n",
    "\n",
    "print(\"\\nListe des concepts triée par le score LIG :\")\n",
    "print(ordered_concepts)\n",
    "\n",
    "\n",
    "# --- Chargement du DataFrame de données réelles (df_aug_train) et préparation des colonnes ---\n",
    "# Exemple de chargement : on suppose que df_aug_train contient une colonne pour chacun des concepts,\n",
    "# avec 1 indiquant la présence du concept dans la ligne.\n",
    "# Attention : vous devez adapter le chemin et le nettoyage des colonnes afin que les noms correspondent.\n",
    "\n",
    "# Fonction de nettoyage identique à celle utilisée pour les concepts\n",
    "def clean_concept_name(name):\n",
    "    name = name.replace(\"cos_\", \"\").replace(\"dummy_\", \"\").strip()\n",
    "    name = re.sub(r\"Let me know.*\", \"\", name)\n",
    "    return \" \".join(name.split())\n",
    "\n",
    "# On s'assure que les noms de colonnes du DataFrame sont nettoyés pour correspondre aux noms présents dans ordered_concepts\n",
    "df_aug_train.rename(columns=lambda x: clean_concept_name(x), inplace=True)\n",
    "\n",
    "# --- Calcul de la courbe cumulative à partir de df_aug_train ---\n",
    "counts = []\n",
    "concepts_iter = []\n",
    "for concept in ordered_concepts:\n",
    "    # Ajout du concept courant dans la liste cumulée\n",
    "    concepts_iter.append(concept)\n",
    "    # Sélection des lignes où au moins un des concepts cumulés est présent (valeur == 1)\n",
    "    # On suppose que dans df_aug_train, pour chaque concept, la présence est indiquée par 1 (sinon adapter)\n",
    "    filtered_data = df_aug_train[concepts_iter][df_aug_train[concepts_iter].eq(1).any(axis=1)]\n",
    "    counts.append(len(filtered_data))\n",
    "\n",
    "# Calcul du pourcentage de lignes couvertes par rapport au total du DataFrame\n",
    "percentage = [100 * c / df_aug_train.shape[0] for c in counts]\n",
    "\n",
    "# Positions pour l'axe des x (une par concept)\n",
    "positions = np.arange(len(ordered_concepts))\n",
    "\n",
    "# --- Affichage du graphique combiné ---\n",
    "fig, ax1 = plt.subplots(figsize=(18, 15))\n",
    "\n",
    "# Graphique 1 : Barres verticales pour la valeur moyenne (ici, on peut choisir d'afficher la valeur du score associé du JSON)\n",
    "# Dans cet exemple, on affiche les scores moyens provenant du JSON (extrait du plot_df précédemment)\n",
    "# Si nécessaire, vous pouvez adapter la barre pour afficher une autre valeur.\n",
    "values = plot_df.loc[ordered_concepts, \"LIG score\"]  # ou \"LIG score\", selon le choix\n",
    "\n",
    "ax1.bar(positions, values, color='skyblue', alpha=0.7, label=\"Valeur moyenne des concepts\")\n",
    "ax1.set_ylabel(\"LIG_brute\")\n",
    "ax1.set_xlabel(\"Concepts\")\n",
    "ax1.set_title(\"Valeur moyenne des concepts et couverture cumulative\")\n",
    "ax1.set_xticks(positions)\n",
    "ax1.set_xticklabels(ordered_concepts, rotation=45, ha='right')\n",
    "ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Axe secondaire : Courbe cumulative (pourcentage de lignes couvertes)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(positions, percentage, marker='o', color='darkorange', label=\"Couverture cumulative\")\n",
    "ax2.set_ylabel(\"Pourcentage de lignes avec au moins un concept (%)\", color='darkorange')\n",
    "ax2.tick_params(axis='y', colors='darkorange')\n",
    "ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Légendes combinées\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_coverage = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/sorted_macro_concepts_coverage_{config.annotation}_{config.agg_mode}_{config.agg_scope}_by_lig_brute.json\"\n",
    "    )\n",
    "\n",
    "with open(path_coverage, 'w') as f:\n",
    "    json.dump(score_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "score_dict = dict(zip(ordered_concepts, percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics d'aggregation par cluster\n",
    "\n",
    "path_plot_df = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/combined_score_concept_{config.annotation}_{config.agg_mode}_{config.agg_scope}.csv\"\n",
    "    )\n",
    "\n",
    "plot_df = pd.read_csv(path_plot_df)\n",
    "# plot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cluster_dict_strat_reassignation = f\"{config.SAVE_PATH_CONCEPTS}/cluster_dict_strat_reassignation_C3M.json\"\n",
    "\n",
    "path_cluster_dict_strat_noise_cluster = f\"{config.SAVE_PATH_CONCEPTS}/cluster_dict_strat_noise_cluster_C3M.json\"\n",
    "                                    \n",
    "with open(path_cluster_dict_strat_reassignation, 'r') as f:\n",
    "   cluster_dict_strat_reassignation = json.load(f)\n",
    "\n",
    "with open(path_cluster_dict_strat_noise_cluster, 'r') as f:\n",
    "    cluster_dict_strat_noise_cluster = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_columns_reassignation(concept):\n",
    "    for k, v in cluster_dict_strat_reassignation.items():\n",
    "        if concept in v:\n",
    "            return int(k)\n",
    "    return -1  # ou autre valeur par défaut si pas trouvé\n",
    "\n",
    "def new_columns_noisy(concept):\n",
    "    for k, v in path_cluster_dict_strat_noise_cluster.items():\n",
    "        if concept in v:\n",
    "            return int(k)\n",
    "    return -1  # ou autre valeur par défaut si pas trouvé\n",
    "\n",
    "plot_df['cluster'] = plot_df['concept'].apply(new_columns_reassignation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import hdbscan\n",
    "from umap import UMAP\n",
    "\n",
    "# 0) On met le nom du concept en index\n",
    "plot_df = plot_df.set_index(\"concept\")\n",
    "\n",
    "# 4) Construction : cluster → concepts triés par score\n",
    "cluster_groups = {}\n",
    "for c in sorted(set(set(plot_df['cluster'].to_list()))):\n",
    "    lst = (plot_df\n",
    "           .loc[plot_df[\"cluster\"] == c]\n",
    "           .sort_values(\"LIG score\", ascending=False)\n",
    "           .index\n",
    "           .tolist())\n",
    "    cluster_groups[c] = lst\n",
    "\n",
    "# 5) Sélection round-robin + suivi du coverage par étape\n",
    "total = df_aug_train.shape[0]\n",
    "working = {c: cluster_groups[c].copy() for c in cluster_groups}\n",
    "\n",
    "# Stockage des résultats\n",
    "coverage_evolution = []\n",
    "selected_concepts = []\n",
    "\n",
    "while any(working[c] for c in working):\n",
    "    current_selection = []\n",
    "    for c in sorted(working):\n",
    "        if working[c]:\n",
    "            concept = working[c].pop(0)\n",
    "            current_selection.append(concept)\n",
    "            selected_concepts.append(concept)\n",
    "    \n",
    "    # Calcul du coverage après avoir ajouté ces concepts\n",
    "    mask = df_aug_train[selected_concepts].eq(1).any(axis=1)\n",
    "    coverage = mask.sum() * 100 / total\n",
    "    \n",
    "    coverage_evolution.append((current_selection.copy(), coverage))\n",
    "\n",
    "# 6) Affichage\n",
    "for idx, (concepts, cov) in enumerate(coverage_evolution):\n",
    "    print(f\"Step {idx+1}: Concepts {concepts} → Coverage {cov:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cumuler le nombre total de concepts sélectionnés à chaque étape\n",
    "n_concepts_selected = np.cumsum([len(concepts) for concepts, _ in coverage_evolution])\n",
    "coverage_values = [cov for _, cov in coverage_evolution]\n",
    "\n",
    "# Tracé\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(n_concepts_selected, coverage_values, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Nombre de concepts sélectionnés\")\n",
    "plt.ylabel(\"Coverage cumulé (%)\")\n",
    "plt.title(\"Évolution du coverage en sélection round-robin par cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path_coverage = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs\"\n",
    "    f\"/{config.cavs_type}/sorted_macro_concepts_coverage_MJ_{config.annotation}_{config.agg_mode}_{config.agg_scope}_by_lig_brute.pkl\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Sauvegarde\n",
    "with open(path_coverage, \"wb\") as f:\n",
    "    pickle.dump(coverage_evolution, f)\n",
    "\n",
    "print(\"✅ Fichier 'coverage_evolution.pkl' sauvegardé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(path_coverage, \"rb\") as f:\n",
    "    loaded_coverage_evolution = pickle.load(f)\n",
    "loaded_coverage_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM ERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# On reprend les mêmes df et dicts que précédemment :\n",
    "# - plot_df : DataFrame chargé depuis combined_score_concept_…csv\n",
    "# - df_aug_train : ton DataFrame d'entraînement binaire (colonnes « concept » = 1 ou 0)\n",
    "# - cluster_dict_strat_reassignation et cluster_dict_strat_noise_cluster déjà chargés\n",
    "\n",
    "# Chemins de base\n",
    "base_cavs = Path(config.SAVE_PATH) / \"blue_checkpoints\" / config.model_name / \"cavs\" / config.cavs_type\n",
    "\n",
    "# Chargement de plot_df\n",
    "path_plot_df = base_cavs / f\"combined_score_concept_{config.annotation}_{config.agg_mode}_{config.agg_scope}.csv\"\n",
    "plot_df = pd.read_csv(path_plot_df).set_index(\"concept\")\n",
    "\n",
    "# Préparer les clusters\n",
    "with open(f\"{config.SAVE_PATH_CONCEPTS}/cluster_dict_strat_reassignation_C3M.json\", \"r\") as f:\n",
    "    cluster_reassign = json.load(f)\n",
    "with open(f\"{config.SAVE_PATH_CONCEPTS}/cluster_dict_strat_noise_cluster_C3M.json\", \"r\") as f:\n",
    "    cluster_noise = json.load(f)\n",
    "\n",
    "def assign_cluster(concept, dct):\n",
    "    for k,v in dct.items():\n",
    "        if concept in v:\n",
    "            return int(k)\n",
    "    return -1\n",
    "\n",
    "plot_df[\"cluster\"] = plot_df.index.map(lambda c: assign_cluster(c, cluster_reassign))\n",
    "\n",
    "# Nombre total de lignes pourcentage\n",
    "total = df_aug_train.shape[0]\n",
    "\n",
    "for run in range(10):\n",
    "    # 1) Flipper / mélanger les scores LIG\n",
    "    shuffled = plot_df[\"combined_score_LIG\"].sample(frac=1, random_state=7*run+42).values\n",
    "    plot_df[\"combined_score_LIG_shuffled\"] = shuffled\n",
    "    \n",
    "    # 2) Construire les groupes par cluster, triés par score mélangé\n",
    "    cluster_groups = {\n",
    "        c: plot_df.loc[plot_df.cluster == c]\n",
    "                  .sort_values(\"combined_score_LIG_shuffled\", ascending=False)\n",
    "                  .index\n",
    "                  .tolist()\n",
    "        for c in plot_df[\"cluster\"].unique()\n",
    "    }\n",
    "    working = {c: lst.copy() for c,lst in cluster_groups.items()}\n",
    "\n",
    "    # 3) Round‑robin et calcul du coverage cumulatif\n",
    "    selected = []\n",
    "    coverage_evolution = []\n",
    "    while any(working[c] for c in working):\n",
    "        batch = []\n",
    "        for c in sorted(working):\n",
    "            if working[c]:\n",
    "                batch.append(working[c].pop(0))\n",
    "                selected.append(batch[-1])\n",
    "        # coverage après ajout de batch\n",
    "        mask = df_aug_train[selected].eq(1).any(axis=1)\n",
    "        coverage = mask.sum() * 100.0 / total\n",
    "        coverage_evolution.append((batch.copy(), coverage))\n",
    "\n",
    "    # 4) Sauvegarde du .pkl pour ce run\n",
    "    out_path = base_cavs / f\"sorted_macro_concepts_coverage_MJ_{config.annotation}_{config.agg_mode}_{config.agg_scope}_{run}.pkl\"\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(coverage_evolution, f)\n",
    "        print(coverage_evolution)\n",
    "    print(f\"✅ run {run} → {out_path.name} ({len(coverage_evolution)} étapes) sauvegardé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other analysis : PLAYGROUND "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_heuristique import plot_concept_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# On s'assure que les noms de colonnes du DataFrame sont nettoyés pour correspondre aux noms présents dans ordered_concepts\n",
    "df_aug_train.rename(columns=lambda x: clean_concept_name(x), inplace=True)\n",
    "\n",
    "# Exemple d'appel pour le concept \"famous people\" :\n",
    "plot_concept_threshold(\n",
    "    cosine_df=cosine_df, #or cosine_val pour Plotter sur le val_dataset:\n",
    "    groundtruth_df=df_aug_train,  # DataFrame contenant la ground truth\n",
    "    concept=cosine_df.columns.tolist()[20],\n",
    "    thresholds=thresholds,\n",
    "    gt_prefix=\"dummy_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link between frequences and F1 score (filter method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute concept presence (mean of binary values for each concept)\n",
    "presence_scores = {\n",
    "    concept: df_aug_train[f\"dummy_{concept}\"].mean()\n",
    "    for concept in metrics.keys()\n",
    "    if f\"dummy_{concept}\" in df_aug_train.columns\n",
    "}\n",
    "\n",
    "# Combine F1 and presence into a DataFrame\n",
    "plot_df = pd.DataFrame({\n",
    "    \"F1 Score\": {k: v['F1'] for k, v in metrics.items()},\n",
    "    \"Presence (Mean)\": presence_scores\n",
    "})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.scatter(plot_df[\"Presence (Mean)\"], plot_df[\"F1 Score\"], color='dodgerblue')\n",
    "for concept, row in plot_df.iterrows():\n",
    "    plt.text(row[\"Presence (Mean)\"], row[\"F1 Score\"], concept, fontsize=9, ha='right')\n",
    "plt.xlabel(\"Presence of Concept (Mean in Training Set)\")\n",
    "plt.ylabel(\"F1 Score (on Validation Set)\")\n",
    "plt.title(\"F1 Score vs Concept Presence\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link between F1 Score (filtering methods) and F1 score (filter method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les perfs train et test des concept et aller dans le fichier new_heuristique pour voir\n",
    "# le lien entre well detected et au final , bien prédit dans le CBM\n",
    "with open(f\"{self.config.SAVE_PATH}blue_checkpoints/{self.config.model_name}/metrics_to_compare_agnews_{config.model_name}.json\", 'r') as f:\n",
    "    cbm_metrics = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine F1 and presence into a DataFrame\n",
    "plot_df_2 = pd.DataFrame({\n",
    "    \"F1 Score\": {k: v['F1'] for k, v in metrics.items()},\n",
    "    \"Presence (Mean)\": cbm_metrics['concept_f1_scores']\n",
    "})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.scatter(plot_df_2[\"Presence (Mean)\"], plot_df_2[\"F1 Score\"], color='dodgerblue')\n",
    "for concept, row in plot_df_2.iterrows():\n",
    "    plt.text(row[\"Presence (Mean)\"], row[\"F1 Score\"], concept, fontsize=9, ha='right')\n",
    "plt.xlabel(\"F1 Score on test for CBM\")\n",
    "plt.ylabel(\"F1 Score (on Validation Set) in the filtering process \")\n",
    "plt.title(\"F1 Score (filtering process) vs F1 score (CBM)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link between frequences and F1 score (CBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute concept presence (mean of binary values for each concept)\n",
    "presence_scores = {\n",
    "    concept: df_aug_train[f\"dummy_{concept}\"].mean()\n",
    "    for concept in metrics.keys()\n",
    "    if f\"dummy_{concept}\" in df_aug_train.columns\n",
    "}\n",
    "\n",
    "# Combine F1 and presence into a DataFrame\n",
    "plot_df = pd.DataFrame({\n",
    "    \"F1 Score (CBM)\": cbm_metrics['concept_f1_scores'],\n",
    "    \"Presence (Mean)\": presence_scores\n",
    "})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.scatter(plot_df[\"Presence (Mean)\"], plot_df[\"F1 Score (CBM)\"], color='dodgerblue')\n",
    "for concept, row in plot_df.iterrows():\n",
    "    plt.text(row[\"Presence (Mean)\"], row[\"F1 Score (CBM)\"], concept, fontsize=9, ha='right')\n",
    "plt.xlabel(\"Presence of Concept (Mean in Training Set)\")\n",
    "plt.ylabel(\"F1 Score (CBM ON TEST)\")\n",
    "plt.title(\"F1 Score vs Concept Presence\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) (test_library_prob) load_model_and_adv_attack_MiB.ipynb",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
