{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0745a076-0599-4a14-a5ff-3e1b0ab4d383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ranking des concepts par LIG-score\n",
    "\n",
    "---\n",
    "Goal of the notebook: Ranker des concepts par LIG-score.\n",
    "\n",
    "Inputs of the notebook:\n",
    "-.\n",
    "Output of the notebook:\n",
    "\n",
    "-.\n",
    "Takeaways: \n",
    "- .\n",
    "- ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b14703-f14b-46cf-af06-cc4193b66bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61184a1f-5298-493b-bf64-b31f6b8d93ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../run_experiments/')\n",
    "sys.path.append('../../run_experiments/scripts')\n",
    "sys.path.append('../../run_experiments/models')\n",
    "sys.path.append('../../run_experiments/data')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import fonction for getting PLM and tokenizer\n",
    "from models.utils import load_model_and_tokenizer\n",
    "\n",
    "# library for managing memory RAM\n",
    "import gc\n",
    "\n",
    "import pickle\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db906d9c-47fa-440b-be84-b2bbb9e940cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f921208-c689-4c81-b68f-22c4922c67ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 0. autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50839d65-be00-4f76-8e12-c122b7b69d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#code for autoreload script associated with jupyter notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7110737a-9a5a-40fb-b645-5c9cd28f652e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.SETUP ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import config\n",
    "from load_config import load_config\n",
    "\n",
    "model_name = 'gemma'    # 'bert-base-uncased' ou 'deberta-large' or 'gemma'\n",
    "dataset    = 'movies'               # 'movies' / 'agnews' / 'dbpedia' / 'medical'/ 'ledgar'/ n24news\n",
    "annotation = 'C3M'       # 'C3M' ou 'our_annotation' ou 'combined_annotation'\n",
    "config = load_config(model_name, dataset)\n",
    "config.annotation = annotation\n",
    "\n",
    "# additionnal parameter for this notebook\n",
    "agg_mode = \"abs\"\n",
    "agg_scope = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Dataframe et dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charger les données de train (facultatif : validation et de test augmented)\n",
    "df_aug_train = pd.read_csv(f\"{config.SAVE_PATH_CONCEPTS}/df_with_topics_v4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82f72545-1d9b-48bd-a777-d66f947d94fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Black Box Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhan/.conda/envs/good_env/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:785: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/bhan/.conda/envs/good_env/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf1fb33be2946d88b56e7cabe3c2dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhan/Yann_CBM/Launch/notebooks/../run_experiments/models/BaselineModel.py:199: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.classifier.load_state_dict(torch.load(f\"{self.save_path}blue_checkpoints/{self.config.model_name}/BaselineModel/{self.embedder_model_name}_classifier_state_dict.pth\"))\n",
      "/home/bhan/Yann_CBM/Launch/notebooks/../run_experiments/models/BaselineModel.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.embedder_model.load_state_dict(torch.load(f\"{self.save_path}blue_checkpoints/{self.config.model_name}/BaselineModel/{self.embedder_model_name}_state_dict.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucune performance enregistrée trouvée.\n",
      "Tous les paramètres sont maintenant en float64 : True\n"
     ]
    }
   ],
   "source": [
    "from models.BaselineModel import BaselineModel\n",
    "\n",
    "# import the PLM model and tokenizer and bottleneck layer\n",
    "embedder_model, embedder_tokenizer, _, classifier = load_model_and_tokenizer(config, n_concepts = 4)\n",
    "\n",
    "black_box_model = BaselineModel(embedder_model, classifier, None, None, None, config, save_path = config.SAVE_PATH)\n",
    "black_box_model.load_model()\n",
    "\n",
    "# Fonction pour vérifier si les paramètres du modèle sont en float64\n",
    "def check_parameters_dtype(model):\n",
    "    for param in model.parameters():\n",
    "        if param.dtype != torch.float64:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "# Fonction pour convertir les paramètres du modèle en float64\n",
    "def convert_parameters_to_float64(model):\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.double()\n",
    "\n",
    "# Supposons que `black_box_model` est l'instance du modèle\n",
    "convert_parameters_to_float64(black_box_model)\n",
    "\n",
    "# Vérifier si tous les paramètres sont maintenant en float64\n",
    "is_float64 = check_parameters_dtype(black_box_model)\n",
    "\n",
    "print(f\"Tous les paramètres sont maintenant en float64 : {is_float64}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define function to wrap LIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Fonction 0 : Wrapper le modèle black box pour captum\n",
    "##############################################\n",
    "# !!! attention black_box_model utiliser comme variable local ci-dessous !!!\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "if config.model_name == 'gemma':\n",
    "    def forward_LIG_black_box(input_ids, attention_mask=None):\n",
    "        outputs = black_box_model.embedder_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs[0][:, -1, :]\n",
    "        logits = black_box_model.classifier(pooled)\n",
    "    lig = LayerIntegratedGradients(\n",
    "        forward_LIG_black_box,\n",
    "        layer=black_box_model.embedder_model.layers[-1],\n",
    "        # attribute_to_layer_input=True \n",
    "    )\n",
    "else:   \n",
    "    def forward_LIG_black_box(input_ids, attention_mask):\n",
    "        outputs = black_box_model.embedder_model(input_ids=input_ids.to(config.device), attention_mask=attention_mask.to(config.device))\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = black_box_model.classifier(pooled_output)\n",
    "        return logits\n",
    "    lig = LayerIntegratedGradients(\n",
    "        forward_LIG_black_box,\n",
    "        layer = black_box_model.embedder_model.encoder.layer[-1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from LIG_ranking import postprocess_cosine, compute_cosine_similarities\n",
    "if config.model_name == 'gemma':\n",
    "    from LIG_ranking import compute_attributions_on_gemma as compute_attributions\n",
    "else:\n",
    "    from LIG_ranking import compute_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mode=\"clip\", agg_scope=\"present\"):\n",
    "    BATCH_SIZE = 1\n",
    "    # Chemin pour sauvegarder le DataFrame des attributions\n",
    "    path_attr = f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs/{config.cavs_type}/attributions_df_{config.cavs_type}_{config.annotation}.pkl\"\n",
    "    os.makedirs(os.path.dirname(path_attr), exist_ok=True)\n",
    "    \n",
    "    # Si le fichier existe déjà, on le charge, sinon on calcule et on sauvegarde\n",
    "    if os.path.exists(path_attr):\n",
    "        print(\"attributions_df déjà présent, chargement...\")\n",
    "        with open(path_attr, \"rb\") as f:\n",
    "            attributions_df = pickle.load(f)\n",
    "    else:\n",
    "        attributions_df = compute_attributions(df_aug_train, BATCH_SIZE, embedder_tokenizer, lig, config.device)\n",
    "        with open(path_attr, \"wb\") as f:\n",
    "            pickle.dump(attributions_df, f)\n",
    "        print(\"attributions_df sauvegardé à\", path_attr)\n",
    "    \n",
    "    # Chargement des vecteurs CAV à partir d'un fichier JSON et conversion sur GPU\n",
    "    file_path = f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs/{config.cavs_type}/cavs_{config.cavs_type}_{config.annotation}.json\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            cavs_vectors = json.load(f)\n",
    "        cavs = {k: torch.tensor(v, dtype=torch.float32).to(config.device) for k, v in cavs_vectors.items()}\n",
    "        print(\"cavs chargés à partir de\", file_path)\n",
    "    else:\n",
    "        print(\"Fichier cavs introuvable :\", file_path)\n",
    "        return  # Arrêter l'exécution si le fichier n'existe pas\n",
    "\n",
    "    # Chemin pour sauvegarder le DataFrame mis à jour avec les similarités cosinus\n",
    "    path_cosine_df = f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/cavs/{config.cavs_type}/df_aug_train_updated_{config.annotation}.pkl\"\n",
    "    if os.path.exists(path_cosine_df):\n",
    "        print(\"DataFrame mis à jour déjà présent, chargement...\")\n",
    "        with open(path_cosine_df, \"rb\") as f:\n",
    "            df_aug_train_updated = pickle.load(f)\n",
    "    else:\n",
    "        df_aug_train_updated = compute_cosine_similarities(attributions_df, df_aug_train, cavs, config.device)\n",
    "        with open(path_cosine_df, \"wb\") as f:\n",
    "            pickle.dump(df_aug_train_updated, f)\n",
    "        print(\"df_aug_train_updated sauvegardé à\", path_cosine_df)\n",
    "\n",
    "    # Chemin pour sauvegarder le tri des concepts par moyenne des similarités cosinus\n",
    "    file_path_2 = (f\"{config.SAVE_PATH}/blue_checkpoints/{config.model_name}/\"\n",
    "                    f\"cavs/{config.cavs_type}/sorted_macro_concepts_cosine_sm_{config.cavs_type}_{config.annotation}_{mode}_{agg_scope}.json\"\n",
    "                    )\n",
    "    if os.path.exists(file_path_2):\n",
    "        with open(file_path_2, \"r\") as f:\n",
    "            sorted_concepts = json.load(f)\n",
    "        print(\"Fichier sorted_macro_concepts_cosine_sm déjà présent, chargement...\")\n",
    "    else:\n",
    "        df_aug_train_updated, sorted_concepts = postprocess_cosine(\n",
    "            df_aug_train_updated, list(cavs.keys()), mode=mode, agg_scope=agg_scope\n",
    "        )\n",
    "    with open(file_path_2, \"w\") as f:\n",
    "        json.dump(sorted_concepts, f, indent=4)\n",
    "    print(\"Fichier sorted_macro_concepts_cosine_sm sauvegardé à\", file_path_2)\n",
    "\n",
    "    # Libération finale de la mémoire\n",
    "    del attributions_df, cavs, cavs_vectors\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributions_df déjà présent, chargement...\n",
      "cavs chargés à partir de /home/bhan/Yann_CBM/Launch/dbfs/results_movies//blue_checkpoints/gemma/cavs/mean/cavs_mean_C3M.json\n",
      "DataFrame mis à jour déjà présent, chargement...\n",
      "Fichier sorted_macro_concepts_cosine_sm déjà présent, chargement...\n",
      "Fichier sorted_macro_concepts_cosine_sm sauvegardé à /home/bhan/Yann_CBM/Launch/dbfs/results_movies//blue_checkpoints/gemma/cavs/mean/sorted_macro_concepts_cosine_sm_mean_C3M_abs_all.json\n"
     ]
    }
   ],
   "source": [
    "main(mode=\"abs\", agg_scope=\"all\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) (test_library_prob) load_model_and_adv_attack_MiB.ipynb",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
